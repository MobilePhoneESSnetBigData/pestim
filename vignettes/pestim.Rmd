---
title: "A hierarchical model to estimate population with pestim"
date: "`r Sys.Date()`"
output: 
  html_vignette:
    toc: yes
    toc_depth: 2
#    number_sections: true
    df_print: kable
author: Ciprian Alexandru
vignette: >
  %\VignetteIndexEntry{A hierarchical model to estimate population with pestim}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{amsthm}
references:
    - id: ManNacAlb15a
      title: Introduction to ecological sampling
      editor:
      - family: Manly
        given: B.F.J.
      - family: Navarro Alberto
        given: J.A.
      publisher: CRC Press
      issued:
        year: 2015
    - id: RoyDor08a
      title: Hierarchical modeling and inference in ecology
      author:
      - family: Royle
        given: J.A.
      - family: Dorazio
        given: R.M.
      publisher: Academic Press
      issued:
        year: 2008
    - id: BDA3
      title: Bayesian Data Analysis (3rd ed)
      author:
      - family: Gelman
        given: A.
      - family: Carlin
        given: J.B.
      - family: Stern
        given: H.S.
      - family: Dunson
        given: D.B.
      - family: Vehtari
        given: A.
      - family: Rubin
        given: D.B.
      publisher: CRC Press
      issued:
        year: 2013
    - id: FlaSed95a
      title: Mellin transforms and asymptotics&#58; Finite differences and Rice's integrals
      author:
      - family: Flajolet
        given: P.
      - family: Sedgewick
        given: R.
      container-title: Theoretical Computer Science 144, 101-124
      volume: 144
      page: 101-124
      issued: 
        year: 1995
    - id: GraKnuPat96a
      title: Concrete Mathematics (2nd ed.)
      author:
      - family: Graham
        given: R.L.
      - family: Knuth
        given: D.E.
      - family: Patashnik
        given: O.
      publisher: Addison-Wesley
      issued:
        year: 1996
    - id: Joh02a
      title: The curious history of FaÃ  di Bruno's formula
      author:
      - family: Johnson
        given: W.P.
      container-title: American Mathematical Monthly 109, 217-234
      volume: 109
      page: 217-234
      issued:
        year: 2002
        month: March
    - id: BroChu03a
      title: Complex variables and applications (7th ed.)
      author:
      - family: Brown
        given: J.W.
      - family: Churchill
        given: R.V.
      issued:
        year: 2003
    - id: GraRyz07a
      title: Tables of Integrals, Series, and Products (7th ed.)
      author:
      - family: Gradshteyn
        given: I.S.
      - family: Ryzhik
        given: I.M.
      issued:
        year: 2007
    - id: Dev86a
      title: Non-uniform random variable generation
      author:
      - family: Devroye
        given: L.
      publisher: Springer
      issued:
        year: 1986
    - id: Q2016
      title: Assessing the Quality of Mobile Phone Data as a Source of Statistics
      author:
      - family: De Meersman
        given: F.
      - family: Seynaeve
        given: G.
      - family: Debusschere
        given: M.
      - family: Lusyne
        given: P.
      - family: Dewitte
        given: P.
      - family: Baeyens
        given: Y.
      - family: Wirthmann
        given: A.
      - family: Demunter
        given: C.
      - family: Reis
        given: F.
      - family: Reuter
        given: H.I. 
      container-title: Q2016 Conference
      volume: 
      page: 
      issued: 
        month: June
        year: 2016
    - id: WP5Del11
      title: Current status of access to mobile phone data in the ESS 
      author: 
        family: ESSnet on Big Data WP5
      issued:
        year: 2017
    - id: WP5IntDoc4
      title: A simple hierarchical model to estimate population counts from aggregated mobile phone data
      author: 
        family: ESSnet on Big Data WP5
      issued:
        year: 2017
       
csl: chicago-author-date.csl
---

\newtheorem{lemma}{Lemma}


```{r, echo = FALSE, message = FALSE}
#```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
```

# Introduction

This vignette explains how to estimate population with `pestim` package.

This package include functions for a hierarchical model to estimate population counts using a combination of aggregated mobile phone data and official data both at a given time instant and along a sequence of time periods. 

Also, we show how to extend the estimates along a sequence of time instants, not just the initial one.

This approach is using the ecological sampling techniques to estimate population counts (see e.g. Manly and Navarro Alberto (2015)).
In particular, we follow the work by Royle and Dorazio (2008).

In this first proposal we envisage the inference exercise from both mobile phone and official data to population counts as a two-step process. Firstly using the official data we assume that they correspond to a given time instant $t_{0}$ so that at this time instant both mobile phone and official data are used to infer the population counts in each territorial cell. Then, for later time instants, we shall infer transition probabilities using only mobile phone data to study the spatial and time evolution of the population.

For the time being we will concentrate on mathematical aspects of the model for the first step and a preliminary prototyping implementation to assess the performance of the model. In separate documents we will consider the details about the second step and all the software implementation.

# The hierarchical model

## Setting up the model

Firstly we set the notation. We shall denote by $\mathbf{N}^{\textrm{MNO}}=(N_{1}^{\textrm{MNO}}, \dots, N_{I}^{\textrm{MNO}})^{T}$ the population counts according to the mobile devices reported by the mobile network operator in each territorial cell $i\in\mathcal{I}=\{1,\dots,I\}$ (i.e. the aggregated mobile phone data). These can refer to general population counts, tourist counts, commuters counts, etc. This is considered as an input in the model. Along with the efforts to gain access to mobile phone data NSIs will have also to develop methodologies to obtain these aggregated data out of statistical microdata. Following the generic bottom-up approach of the ESSnet we will concentrate on the part of the process upon which we can carry out concrete analyses. The issue of the access to these microdata must be conveniently solved to follow an empirical approach on the processing of microdata. For the present project we will cover this part of the production process with the internal technical reports by Positium.

In the model, we will make use as auxiliary information of the official population register number of individuals $\mathbf{N}^{\textrm{REG}}=(N_{1}^{\textrm{REG}}, \dots, N_{I}^{\textrm{REG}})^{T}$ in each of the cells or some equivalent survey or administrative source. The goal is to provide estimates for the actual population counts $\mathbf{N}=(N_{1}, \dots, N_{I})^{T}$ combining both data sources. The interplay between official data and mobile phone data will also be discussed elsewhere (see also the internal documents of the WP5).

More rigorously, aiming at the quality assessment of the estimation procedure, we will produce a probability distribution for the number of individuals of the target population in each cell $i$ using both the mobile phone and official population data as inputs (see figure \ref{Tool}). The posterior probability distribution $\mathbb{P}\left(\mathbf{N}|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textrm{REG}}\right)$ will allow us to assess the uncertainty in the output estimates.

<figure>
  <img src="img/Tool1.png" alt="Schematic diagram for the output intended using mobile phone and official population data" width="100%" >
</figure>

We propose the hierarchical model given by:

\begin{align}
N_{i}^{\textrm{MNO}}&\simeq\textrm{Bin}\left(N_{i}, p_{i}\right),\qquad N_{i}^{\textrm{MNO}}\perp N_{j}^{\textrm{MNO}},\quad i\neq j=1,\dots,I\\
N_{i}&\simeq\textrm{Po}\left(\lambda_{i}\right),\qquad N_{i}\perp N_{j},\quad i\neq j=1,\dots,I\nonumber\\
p_{i}&\simeq\textrm{Beta}\left(\alpha_{i},\beta_{i}\right),\qquad p_{i}\perp p_{j}\quad i\neq j=1,\dots,I\nonumber\\
\left(\alpha_{i}, \beta_{i}\right)&\simeq \frac{f_{1}(\frac{\alpha_{i}}{\alpha_{i}+\beta_{i}}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})\cdot f_{2}(\alpha_{i}+\beta_{i}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})}{\alpha_{i}+\beta_{i}},\qquad (\alpha_{i},\beta_{i})\perp(\alpha_{j},\beta_{j}),\quad i\neq j=1,\dots,I\nonumber\\
\lambda_{i}&\simeq f_{3}(\lambda_{i}; N_{i}^{\textrm{REG}}, \mathbf{z})\quad (\lambda_{i} > 0, \lambda_{i}\perp\lambda_{j}), \quad i=1,\dots,I.\nonumber
\end{align}

## Interpretating the model

The interpretation of the model is more or less straightforward. If in a territorial cell $i$ there are $N_{i}$ individuals and we have an independent detection probability $p_{i}$ for each individual through the mobile telecommunication network, then we will detect $N_{i}^{\textrm{MNO}}$ individuals according to the aggregated mobile phone data naturally following a binomial distribution. 

Now, the number of individuals $N_{i}$ in each cell can be understood as a Poisson random variable (potentially arising from an underlying birth-death Poisson process). These variables are pairwise independent and depend on unknown independent parameters $\lambda_{i}$. For the time being we will keep the model as simple as possible to test a first proof of concept.

Now, the detection probabilities $p_{i}$ in our mobile phone setting differs from the usual ecological setting. In the latter, the field work (observation sites, visual techniques, \dots) allows us to propose a model for these probabilities according to the measurement process. In the telecommunication setting, in principle, the measurement process in cell $i$ is always successful provided that a subscriber interacting with the network is within the territorial cell $i$. Thus at the given instant of time $t_{0}$ the detection probabilities $p_{i}$ amount to establish the proportion of individuals of interest at each cell $i$ being detected by the MNO's cellular network. In other words, $p_{i}$ are the proportions of individuals detected by the MNO at time $t_{0}$ in each cell $i$.

It is interesting to make a short reflection about these proportions $p_{i}$ and the so-called local market shares. The latter are the number of subscribers of a given MNO in a cell $i$ and they are sometimes considered as an important piece of information in performing the inference exercise from mobile phone data to the target population. We must stress that, in our view, it is not the concept of market share which is important but that of the actual proportion of individuals detected by the network. As an illustrative example, a call between a subscriber in a cell $i$ and a non-subscriber in another cell $j$ of a given MNO is certainly detected by the network in \emph{\textbf{both cells}}, thus potentially being part of the aggregated data $N_{i}^{\textrm{MNO}}$ and $N_{j}^{\textrm{MNO}}$. This is a clear example of why having knowledge of the preprocessing and aggregation procedures from microdata is important for the final results.

We will model the detection probabilities $p_{i}$ to account for the uncertainty we have in these quantities. Thus they are modelled as beta random variables with parameters $\alpha_{i}, \beta_{i}$ independently in each cell. The prior distribution of the beta distribution parameters $\alpha_{i}, \beta_{i}$ arises from the following reasoning. We assume that $\frac{\alpha_{i}}{\alpha_{i} + \beta_{i}}$ and $\alpha_{i} + \beta_{i}$ distribute independently according to $\frac{\alpha_{i}}{\alpha_{i} + \beta_{i}}\simeq f_{1}(\frac{\alpha_{i}}{\alpha_{i}+\beta_{i}}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})$  and $\alpha_{i} + \beta_{i}\simeq f_{2}(\alpha_{i}+\beta_{i}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})$, where $f_{1}$ and $f_{2}$ are respective weakly informative prior distributions for $\frac{\alpha}{\alpha + \beta}$ and $\alpha + \beta$. Notice that we have again made use of the auxiliary information coming from the population register ($\mathbf{N}^{\textrm{REG}}$) and any other auxiliary information $\mathbf{z}$. The quantity $\alpha_{i}/(\alpha_{i}+\beta_{i})$ can be understood as a priori proportions of individuals detected by the MNO in cell $i$ (e.g. should we have no information, then $f_{1}=\textrm{Unif}[0,1]$). The parameters $\alpha_{i}+\beta_{i}$ can be essentially understood as the population size of each cell $N_{i}$ (thus with support in $(0,\infty)$) upon which the detection is executed at that time instant. For example, we may assume $f_{2}$ to be a gamma distribution with parameters $(N_{i}^{\textrm{MNO}} + 1, \frac{N_{i}^{\textrm{MNO}}}{N_{i}^{\textrm{REG}}})$. In this way, the most probable value for the sample size is $N_{i}^{\textrm{REG}}$ in consonance with the preceding hypothesis for $N_{i}$.

Finally, the parameters $\lambda_{i}$ are modeled with another weakly information prior $f_{3}$ which may incorporate the information we have from the population register or similar sources. Notice that the only a priori information incorporated is coming from this auxiliary source.

There is a clear abuse of notation by denoting both random variables and their realization in the same way (the context will make this clear).  $\mathbf{N}^{\textrm{REG}}$ are treated as fixed parameters in the current model. By relaxing this and modelling also $\mathbf{N}^{\textbf{REG}}$ we can pave the way to account for proposing more complex models for the uncertainty (possible non-sampling errors) in the official population figures for this estimation procedure.

Notice also that the cells are treated independently leaving the door open for geostatitical considerations naturally accounting for geospatial correlations among the cells. We will concentrate here on the preceding simple model to provide a proof of concept.


## Getting the flavour of the model

To get a flavour of the model, let us make the following simplifying assumption. Let us suppose that the prior distributions $f_{1}$ and $f_{2}$ are degenerate so that equivalently we are assuming that we have full knowledge of the a priori proportion of detected individuals\footnote{For ease of notation we drop out the subscripts $i$ regarding the cells, since they are independent.} $u=\frac{\alpha}{\alpha+\beta}=u^{*}$ and of the population cell size $N^{*}=\alpha+\beta$ whose proportion of subscribers is detected by our MNO.

Then it is straighforward to show that the unnormalized posterior probability density $\mathbb{P}\left(\lambda|N^{\textrm{MNO}}; N^{\textrm{REG}}\right)$ is given by 

\begin{align}
\mathbb{P}\left(\lambda|N^{\textrm{MNO}}; N^{\textrm{REG}}\right)&\propto f_{3}(\lambda; N^{\textrm{REG}})\cdot\textrm{Po}\left(N^{\textrm{MNO}}; \lambda\right)\cdot\sum_{n=0}^{\infty}\frac{\lambda^{n}}{n!}\frac{B(u^{*}\cdot N^{*} + N^{\textrm{MNO}}, (1-u^{*})\cdot N^{*} + n)}{B(u^{*}\cdot N^{*}, (1-u^{*})\cdot N^{*})}\nonumber\\
&{\underset{\sim}{\propto}} f_{3}(\lambda; N^{\textrm{REG}})\cdot \textrm{Po}\left(N^{\textrm{MNO}}; \lambda\right)\cdot \sum_{n=0}^{\infty}\frac{\lambda^{n}}{n!}\cdot u^{*N^{\textrm{MNO}}}\cdot (1- u^{*})^{n} \nonumber\\
\label{EBprob}&{\underset{\sim}{\propto}} f_{3}(\lambda; N^{\textrm{REG}})\cdot e^{-\lambda u^{*}}\cdot\frac{(\lambda u^{*})^{N^{\textrm{MNO}}}}{N^{\textrm{MNO}}!},
\end{align}

\noindent where we have used the approximation $\frac{\Gamma(x + a)}{\Gamma(x)}\approx x^{a}$ (which can be proved using Stirling's approximation) and where $\textrm{Po}(N; \lambda)$ denotes the probability function of a Poisson random variable $N$ with parameter $\lambda$..

In the case of noninformative prior $f_{3}\propto 1$ the posterior \eqref{EBprob} corresponds to a gamma distribution for $\lambda$ with parameters  $N^{\textrm{MNO}} + 1$ and $u^{*}$. The mode of this distribution (thus the most probable value for $\lambda$) is $\frac{N^{\textrm{MNO}}}{u^{*}}$. In turn, the most probable value for $N$ in the model is $\lfloor\lambda\rfloor = \lfloor\frac{N^{\textrm{MNO}}}{u^{*}}\rfloor$. With the due rigorous proviso, $u^{*}$ can be somehow understood as a sampling weight connecting the population of detected individuals through the mobile phone network with the target population.

Suppose now that we assume a prior gamma distribution $\lambda\simeq \Gamma(\alpha + 1, N^{\textrm{REG}}/\alpha)$, where $\alpha>0$. Then the posterior \eqref{EBprob} is again a gamma distribution now with parameters $\Gamma(\alpha + N^{\textrm{MNO}} + 1, u^{*} + \frac{\alpha}{N^{\textrm{REG}}})$. The most probable value then for $\lambda$ is $\frac{ N^{\textrm{MNO}} + \alpha}{u^{*} + \frac{\alpha}{N^{\textrm{REG}}}}$ and for $N$ is $\lfloor\frac{N^{\textrm{MNO}} + \alpha}{u^{*} + \frac{\alpha}{N^{\textrm{REG}}}}\rfloor$, which can be written as 

\begin{align}
\widehat{N}&=\left\lfloor \frac{u^{*}\cdot N^{\textrm{REG}}}{\alpha + u^{*}\cdot N^{\textrm{REG}}} \cdot \frac{N^{\textrm{MNO}}}{u^{*}} + \frac{\alpha}{\alpha + u^{*}\cdot N^{\textrm{REG}}}\cdot N^{\textrm{REG}}\right\rfloor\nonumber\\
&\approx \frac{u^{*}\cdot N^{\textrm{REG}}}{\alpha + u^{*}\cdot N^{\textrm{REG}}} \cdot \left\lfloor\frac{N^{\textrm{MNO}}}{u^{*}}\right\rfloor + \frac{\alpha}{\alpha + u^{*}\cdot N^{\textrm{REG}}}\cdot  N^{\textrm{REG}}
\end{align}

The estimate is thus an accurately approximate convex combination of both extremes: (i) having no auxiliary information at all about the population register and (ii) using only the information from the population register. The relative weight between these two components is provided by the parameter $\alpha$.

The full Bayesian approach in the forthcoming sections incorporate our uncertainty in the knowledge of the hyperparameters (especially of $u=\frac{\alpha}{\alpha + \beta}$ and $v = \alpha + \beta$), since we do not know with certainty the values of the proportion of individuals and of the actual population size of each cell upon which the detection is executed.

# Posterior distribution for the target population

The quantity of interest is the target population counts $\mathbf{N}=(N_{1},\dots,N_{I})^{T}$ in each cell $i$. To account for uncertainty (thus for accuracy estimation and quality measures) we will follow a Bayesian approach. Notice that we can leverage the prior information we have by choosing the probability distribution $f_{1}$, $f_{2}$ and $f_{3}$. Should we choose very weakly informative priors, final estimates would not a priori differ very much from the frequentist approach. We shall not make considerations about the philosophical differences between both approaches. We will keep pragmatic and assess the final results according to our simulations. The choice of the Bayesian approach allows us to account for the inference and the assessment of the uncertainty, hence of quality, thus complying with the goals of the project.

Thus we must find the posterior distribution $\mathbb{P}\left(\mathbf{N}\big|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textbf{REG}}\right)$ or equivalently the marginal distributions $\mathbb{P}\left(N_{i}\big|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textbf{REG}}\right)$. We drop the subscripts since the treatment of each cell is equivalent and independent of each other. As usual, we will focus on the unnormalised version of the involved probabilities. We make use of the hierarchy:

\begin{align}
\mathbb{P}\left(N\big|N^{\textrm{MNO}};N^{\textbf{REG}}\right)&\propto\int_{0}^{\infty} d\lambda\quad\mathbb{P}\left(N|\lambda\right)\mathbb{P}\left(\lambda\big|\mathbf{N}^{\textrm{MNO}};N^{\textbf{REG}}\right)\nonumber\\
&\propto \int_{0}^{\infty}d\lambda\quad\mathbb{P}\left(\lambda\big|N^{\textrm{MNO}};N^{\textbf{REG}}\right)\cdot e^{-\lambda}\cdot\frac{\left(\lambda  \right)^{N}}{N!}\nonumber\\
\label{PostProbN}&\propto \int_{0}^{\infty}d\lambda\quad\mathbb{P}\left(\lambda\big|N^{\textrm{MNO}};N^{\textbf{REG}}\right)\cdot\textrm{Po}(N; \lambda),
\end{align}

As expected, we need the posterior distribution for the hyperparameters, which moreover will allow us also to practise inference and simulations and to assess the quality of the model. 

As we have stated in the preceding section notice that being $N$ a Poisson random variable, the most probable value for $N$ is given by $\lfloor \lambda\rfloor$. Thus the posterior distribution for the hyperparameter $\lambda$ will allow us to provide a point estimator for $N$ (indeed as many as we want: mode, mean, median, \dots).

# Posterior distribution for the hyperparameters


## A first analytical expression 

We will try to be analytical as far as possible, leaving numerical computations for later developments. To compute the posterior $\mathbb{P}\left(\lambda \big|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textbf{REG}}\right)$ we proceed as always (see e.g. @BDA3):

\begin{align}
\mathbb{P}\left(\lambda\big|N^{\textrm{MNO}};N^{\textbf{REG}}\right)&\propto\mathbb{P}\left(N^{\textrm{MNO}}|\lambda;N^{\textbf{REG}}\right)\nonumber\\
&\propto\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\int_{0}^{1}dp\sum_{n = N^{\textrm{MNO}}}^{\infty}\mathbb{P}\left(N^{MNO}|p, N\right)\mathbb{P}\left(N|\lambda;N^{\textrm{REG}}\right)\mathbb{P}\left(p|\alpha,\beta\right)\mathbb{P}\left(\alpha,\beta; N^{\textrm{REG}}\right)\mathbb{P}\left(\lambda\right)\nonumber\\
&\propto\mathbb{P}\left(\lambda\right)\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\int_{0}^{1}dp\sum_{n = N^{\textrm{MNO}}}^{\infty}\binom{n}{N^{\textrm{MNO}}}p^{N^{\textrm{MNO}}}(1-p)^{n-N^{\textrm{MNO}}}e^{-\lambda}\frac{\left(\lambda\right)^{n}}{n!}\frac{p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha, \beta)}\frac{f_{1}(\frac{\alpha}{\alpha+\beta}; \mathbf{N}^{\textrm{REG}})\cdot f_{2}(\alpha+\beta; \mathbf{N}^{\textrm{REG}})}{\alpha+\beta}
\end{align}

Simplifying we arrive at

\begin{align}
\mathbb{P}\left(\lambda\big|N^{\textrm{MNO}};N^{\textbf{REG}}\right)&\propto\nonumber\\
&\mathbb{P}\left(\lambda\right)\sum_{n = N^{\textrm{MNO}}}^{\infty}\binom{n}{N^{\textrm{MNO}}}e^{-\lambda}\frac{\lambda^{n}}{n!}\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\frac{f_{1}(\frac{\alpha}{\alpha+\beta}; N^{\textrm{REG}})\cdot f_{2}(\alpha+\beta; N^{\textrm{REG}})}{\alpha+\beta}\frac{B\left(\alpha+N^{\textrm{MNO}}, \beta+n-N^{\textrm{MNO}}\right)}{B\left(\alpha, \beta\right)}\nonumber\\
&\propto\mathbb{P}\left(\lambda\right)\sum_{n = N^{\textrm{MNO}}}^{\infty}\binom{n}{N^{\textrm{MNO}}}e^{-\lambda}\frac{\lambda^{n}}{n!}I_{N^{\textrm{MNO}}, n-N^{\textrm{MNO}}}(N^{\textrm{REG}})\nonumber\\
&\propto\mathbb{P}\left(\lambda\right)\textrm{Po}(N^{\textrm{MNO}}; \lambda)\sum_{n = 0}^{\infty}\frac{\lambda^{n}}{n!}I_{N^{\textrm{MNO}}, n}(N^{\textrm{REG}})\nonumber\\
\label{Hyper}&\propto\mathbb{P}\left(\lambda\right)\cdot \textrm{Po}(N^{\textrm{MNO}}; \lambda) \cdot S\left(\lambda, N^{\textrm{MNO}}, N^{REG}\right),
\end{align}

\noindent where we have defined 

\begin{align}
I_{N^{\textrm{MNO}}, n}(N^{\textrm{REG}})&=\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\ \frac{f_{1}(\frac{\alpha}{\alpha+\beta}; N^{\textrm{REG}})\cdot f_{2}(\alpha+\beta; N^{\textrm{REG}}}{\alpha+\beta}\frac{B\left(\alpha+N^{\textrm{MNO}}, \beta+n- N^{\textrm{MNO}}\right)}{B\left(\alpha, \beta\right)},\\
S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}}) &= \sum_{n = 0}^{\infty}\frac{\lambda^{n}}{n!}I_{N^{\textrm{MNO}}, n}(N^{\textrm{REG}}).
\end{align}

Everything is thus reduced to the computation of the expression $S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})$ (the integral and the sum of the series).

## Computation of $S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})$

We include here the different attempts conducted to compute $S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})$ analytically.


### Approach 1

In this approach we first compute the integral $I_{N^{\textrm{MNO}}, n}(N^{\textrm{REG}})$ and then we sum up the series. We perform the change of variables $u = \frac{\alpha}{\alpha + \beta}, v = \alpha + \beta$ so that the integral transforms into 

\begin{subequations}
\begin{align}
\label{MC}I_{n,m}(N^{\textrm{REG}})&= \int_{0}^{\infty}dv f_{2}(v)\int_{0}^{1}du\ f_{1}(u)\frac{B(u\cdot v + n, (1-u)\cdot v + m)}{B(u\cdot v, (1-u)\cdot v)}\\
&=\int_{0}^{\infty}dv\ f_{2}(v)\frac{\Gamma(v)}{\Gamma(v + n + m)}\int_{0}^{1}du\ f_{1}(u)\frac{\Gamma(u\cdot v + n)}{\Gamma(u\cdot v)}\frac{\Gamma((1-u)\cdot v + m)}{\Gamma((1-u)\cdot v)}\nonumber\\
\label{Analyt}&=\int_{0}^{\infty}dv\ f_{2}(v)\frac{\Gamma(v)}{\Gamma(v + n + m)}\int_{0}^{v}dt\ f_{1}(t/v)\frac{\Gamma(t + n)}{\Gamma(t)}\frac{\Gamma(v - t + m)}{\Gamma(v - t)}
\end{align}
\end{subequations}

Expression \eqref{MC} will allow us to compute the integral via Monte Carlo methods. We will pursue the analytical computation using expression \eqref{Analyt}. The inner integral can be computed expressing the integrand in terms of Stirling numbers of the first kind (see e.g. @GraKnuPat96a):

\begin{align}
\int_{0}^{v}dt f_{1}(t/v)\frac{\Gamma(t + n)}{\Gamma(t)}\frac{\Gamma(v - t + m)}{\Gamma(v - t)}&=\int_{0}^{v}dt (t+n-1)\cdots(t+1)t \cdot (m-1+v-t)\cdots(1+v-t)(v-t)\nonumber\\
&=\int_{0}^{v}dt f_{1}(t/v) \prod_{k=0}^{n-1}(t+k)\prod_{l=0}^{m-1}(v-t+l)\nonumber\\
& = \int_{0}^{v}dt f_{1}(t/v) \sum_{k=0}^{n}\left[\begin{matrix}n\\k\end{matrix}\right]t^{k}\sum_{l=0}^{m}\left[\begin{matrix}m\\l\end{matrix}\right](v-t)^{l}\nonumber\\
& = \sum_{k=0}^{n}\sum_{l=0}^{m}\left[\begin{matrix}n\\k\end{matrix}\right]\left[\begin{matrix}m\\l\end{matrix}\right]v^{k+l+1}\int_{0}^{1}f_{1}(x)x^{k}(1-x)^{l}dx\nonumber\\
& = \sum_{k=0}^{n}\sum_{l=0}^{n}\left[\begin{matrix}n\\k\end{matrix}\right]\left[\begin{matrix}m\\l\end{matrix}\right]\bar{B}(k+1,l+1)v^{k+l+1},
\end{align}

\noindent where $\left[\begin{matrix}n\\k\end{matrix}\right]$ denotes the unsigned Stirling numbers of the first kind and $\bar{B}(k+1, l+1)=\int_{0}^{1}f_{1}(x)x^{k}(1-x)^{l}dx$ (notice that for $f_{1}=\textrm{Unif(0,1)}$ we have $\bar{B}=B$). Then we can write

\begin{align}
I_{n,m}(N^{\textrm{REG}})&=\sum_{k=0}^{n}\sum_{l=0}^{m}\left[\begin{matrix}n\\k\end{matrix}\right]\left[\begin{matrix}m\\l\end{matrix}\right]\bar{B}(k+1,l+1)\int_{0}^{\infty}dv f_{2}(v; \textbf{N}^{\textrm{REG}}, \textbf{z})\frac{\Gamma(v)}{\Gamma(v + n + m)}v^{k+l+1}
\end{align}

Denoting

\begin{align}
J_{n+m,k+l}(N^{\textrm{REG}})&=\int_{0}^{\infty}dv\cdot f_{2}(v; N^{\textrm{REG}})\cdot\frac{v^{k+l}}{\prod_{i=1}^{n+m-1}(v+i)},
\end{align}

we have 

\begin{align}
I_{n,m}(N^{\textrm{REG}})&=\sum_{k=0}^{n}\sum_{l=0}^{m}\left[\begin{matrix}n\\k\end{matrix}\right]\left[\begin{matrix}m\\l\end{matrix}\right]\bar{B}(k+1,l+1)J_{n+m,k+l}(N^{\textrm{REG}})\nonumber\\
&=\sum_{p=0}^{n+m}J_{n+m,p}(N^{\textrm{REG}})\sum_{q=0}^{p}\left[\begin{matrix}n\\q\end{matrix}\right]\left[\begin{matrix}m\\p-q\end{matrix}\right]\bar{B}(q + 1, p - q +1)
\end{align}

Thus we have reduced the integral to the computation of $J_{n+m,p}(N^{\textrm{REG}})$ and $$a_{n,m}(p)=\sum_{q=0}^{p}\left[\begin{matrix}n\\q\end{matrix}\right]\left[\begin{matrix}m\\p-q\end{matrix}\right]\bar{B}(q + 1, p - q +1).$$

These two quantities can be further expressed analytically in some cases (see the appendices). However we do not see an easy computer implementation of these expressions. Thus we opt for Monte Carlo computation. We focus on expression \eqref{MC} to conduct a Monte Carlo evaluation of the integral $I_{n,m}(N^{\textrm{REG}})$. To this end consider the function $g_{n,m}(\mathbf{x})=\frac{B(x_{1}\cdot x_{2} + n, (1-x_{1})\cdot x_{2} + m)}{B(x_{1}\cdot x_{2}, (1-x_{1})\cdot x_{2})}$ and generate $M$ bidimensional random variables $\mathbf{x}\in[0,1]\times\mathbf{R}^{+}$ according to the bidimensional distribution $f_{1}\times f_{2}$. Then, using $f(\mathbf{x})=f_{1}(x_{1})f_{2}(x_{2})$ as importance function, we can write as a first option

\begin{equation}
I_{n,m}(N^{\textrm{REG}})=\lim_{M\to\infty}\frac{1}{M}\sum_{i=1}^{M}g_{n,m}(\mathbf{x}_{i}).
\end{equation}

To accelerate the convergence we make use of stratified importance sampling. To introduce the stratification let us define $H_{1}\cdot H_{2}$ strata as the rectangular domains $[a_{h_{1}-1}, a_{h_{1}}]\times [b_{h_{2} -1}, b_{h_{2}}]$, where $a_{h_{1}}=F_{1}^{-1}\left(h_{1}/H_{1}\right)$ ($h_{1}=1, \dots, H_{1}$) and $b_{h_{2}}=F_{2}^{-1}\left(h_{2}/H\right)$ ($h_{2}=1, \dots, H_{2}$), and $F_{i}$ stands for the distribution function corresponding to the density function $f_{i}$. Defining the importance function in each stratum by $f_{h_{1}h_{2}}= H_{1}\cdot H_{2}\cdot f_{1}\cdot f_{2}$ truncated at $[a_{h_{1}-1}, a_{h_{1}}]\times[b_{h_{2}-1}, b_{h_{2}}]$ and taking equal-size strata $M_{h_{1}h_{2}}=\frac{M}{H_{1}H_{2}}$, then we finally write 

\begin{equation}
I_{n,m}(N^{\textrm{REG}})=\lim_{M\to\infty}\frac{1}{M}\sum_{h_{1}=1}^{H_{1}}\sum_{h_{2}=1}^{H_{2}}\sum_{i_{h_{1}}=1}^{M/H_{2}}\sum_{i_{h_{2}}=1}^{M/H_{1}}g_{n,m}(\mathbf{x}_{i_{h_{1}}i_{h_{2}}})
\end{equation}

\noindent where the random values $\mathbf{x}_{i_{h_{1}}i_{h_{2}}}$ are generated with the corresponding density function $f_{h_{1}h_{2}}$.

Once computed the integrals $I_{N^{\textrm{MNO}}, n}(N^{\textrm{REG}})$, the series $S(\beta_{0}, N^{\textrm{MNO}}, N^{\textrm{REG}})$ is summed up with standard procedures (iteratively or with a tail-recursice algorithm) within a given tolerance $\tau$.

### Approach 2

In this second approach we reverse the order of computation. We first sum up the series and then we compute the integral. Thus we write

\begin{align}
S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})&=\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\ \frac{f_{1}(\frac{\alpha}{\alpha+\beta}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})\cdot f_{2}(\alpha+\beta; \mathbf{N}^{\textrm{REG}}, \mathbf{z})}{\alpha+\beta}\sum_{n=0}^{\infty}\frac{\lambda^{n}}{n!}\frac{B(\alpha + N^{\textrm{MNO}}, \beta + n)}{B(\alpha, \beta)}\nonumber\\
&=\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\ \frac{f_{1}(\frac{\alpha}{\alpha+\beta}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})\cdot f_{2}(\alpha+\beta; \mathbf{N}^{\textrm{REG}}, \mathbf{z})}{(\alpha+\beta) \cdot B(\alpha, \beta)}\int_{0}^{1}dx\ x^{\beta -1}(1-x)^{\alpha + N^{\textrm{MNO}} -1}\sum_{n=0}^{\infty}\frac{(\lambda x)^{n}}{n!}\nonumber\\
&=\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\ \frac{f_{1}(\frac{\alpha}{\alpha+\beta}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})\cdot f_{2}(\alpha+\beta; \mathbf{N}^{\textrm{REG}}, \mathbf{z})}{(\alpha+\beta) \cdot B(\alpha, \beta)}\int_{0}^{1}dx\ e^{\lambda x}x^{\beta -1}(1-x)^{\alpha + N^{\textrm{MNO}} -1}\nonumber\\
&=\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\ \frac{f_{1}(\frac{\alpha}{\alpha+\beta}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})\cdot f_{2}(\alpha+\beta; \mathbf{N}^{\textrm{REG}}, \mathbf{z})}{\alpha+\beta}\frac{B(\alpha + N^{\textrm{MNO}}, \beta)}{B(\alpha, \beta)}\cdot {}_{1}F_{1}(z; \beta, \alpha+\beta +N^{\textrm{MNO}})\nonumber\\
&\equiv\int_{0}^{\infty}\int_{0}^{\infty}d\alpha d\beta\ \frac{f_{1}(\frac{\alpha}{\alpha+\beta}; \mathbf{N}^{\textrm{REG}}, \mathbf{z})\cdot f_{2}(\alpha+\beta; \mathbf{N}^{\textrm{REG}}, \mathbf{z})}{\alpha+\beta}\Phi(\alpha, \beta; \lambda, N^{\textrm{MNO}}, N^{\textrm{REG}}),
\end{align}

\noindent where we have defined $\Phi(\alpha, \beta; \lambda, N^{\textrm{MNO}}, N^{\textrm{REG}}) = \frac{B(\alpha + N^{\textrm{MNO}}, \beta)}{B(\alpha, \beta)}\cdot {}_{1}F_{1}(\lambda; \beta, \alpha+\beta +N^{\textrm{MNO}})$. Now to compute this integral let us change variables as in the preceding section so that:

\begin{align}
S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})&=\int_{0}^{\infty}dv\ f_{2}(v)\int_{0}^{1} du\ f_{1}(u)\cdot \Phi(u\cdot v, (1-u)\cdot v; \lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})\nonumber\\
&=\int_{0}^{\infty}dv\ f_{2}(v)\int_{0}^{1} du\ f_{1}(u) \cdot\bar{\Phi}(u, v; \lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})
\end{align}

\emph{Mutatis mutandi} defining $\mathbf{x}=(u,v)^{T}$ we can apply the same Monte Carlo technique as in the preceding section to arrive at 

\begin{equation}
S(\lambda, N^{\textrm{MNO}}, N^{\textrm{REG}})=\lim_{M\to\infty}\frac{1}{M}\sum_{h_{1}=1}^{H_{1}}\sum_{h_{2}=1}^{H_{2}}\sum_{i_{h_{1}}=1}^{M/H_{2}}\sum_{i_{h_{2}}=1}^{M/H_{1}}\bar{\Phi}(\mathbf{x}_{i_{h_{1}}i_{h_{2}}}; \lambda, N^{\textrm{MNO}}, N^{\textrm{REG}}),
\end{equation}

\noindent where the random values $\mathbf{x}_{i_{h_{1}}i_{h_{2}}}$ are generated with the corresponding density function $f_{h_{1}h_{2}}$.

# Generation of random variables

To conduct simulation studies and carry out the estimation on the number of individuals per cell we need to generate random variables according to the posterior distribution $\mathbb{P}\left(N_{i}|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textrm{REG}}\right)$. We have two options: either we use \eqref{PostProbN} to compute an expression for the probability function of $N_{i}$ or we make use of the hierarchy by generating random values of $\lambda$ according to $\mathbb{P}\left(\lambda|\mathbf{N}^{\textrm{MNO}}; \mathbf{N}^{\textrm{REG}}\right)$ and then use these values to generate $N_{i}$ according to $\textrm{Po}(\lambda)$. We choose the latter. Thus we need to generate random values of the hyperparameter $\lambda$ according to its posterior distribution \eqref{Hyper}:

\begin{align}
\mathbb{P}\left(\lambda\big|N^{\textrm{MNO}};N^{\textbf{REG}}\right)&\propto\mathbb{P}\left(\lambda\right)\cdot\textrm{Po}(N^{\textrm{MNO}}; \lambda) \cdot S\left(\lambda, N^{\textrm{MNO}}, N^{REG}\right).\nonumber
\end{align}

The unnormalized posterior density $\mathbb{P}\left(\lambda|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textrm{REG}}\right)$ does not allow us to find easily the corresponding posterior distribution function to apply the inverse method to generate random variables (see e.g. @Dev86a). We then try to use the acceptance-rejection method. Indeed this method is appropriate to use with unnormalized probability functions.

Let us define $f(x)=\mathbb{P}\left(x|N^{\textrm{MNO}};N^{\textrm{REG}}\right)$. As candidate distribution $g(x)$ we will use a Cauchy distribution $g(x)=\textrm{Cauchy}(x; x_{0}= \lambda^{*}, \sigma)$ truncated at $\mathbb{R}^{+}$ with $\lambda^{*}=\textrm{argmax}_{\lambda\geq 0}f(\lambda)$ (i.e. the mode of $f(\lambda)$). We need to prove rigorously that $f$ is majorized by this candidate distribution $g$. So far, computing tests have been satisfactory. Also we do not have a general recipe for the scale parameter $\sigma$. For the family of prior distributions $\lambda\simeq\Gamma(\alpha + 1, \alpha / N^{\textrm{REG}})$ (with mode $N^{\textrm{REG}}$ and variance $\frac{\alpha + 1}{(\alpha / N^{\textrm{REG}})^2}$) we choose $\sigma = N^{\textrm{REG}}/\sqrt{\alpha}$. The parameter $\alpha$ is a measure of the a priori concentration of $\lambda$ around the value $N^{\textrm{REG}}$ (see next section).

Next we must find $c\in\mathbb{R}$ such that 

\begin{equation}
\inf_{x\geq 0}\frac{c\cdot g(\beta)}{f(x)}\geq 1
\end{equation}

Taking the minimal $c$ for sampling efficiency reasons we have 

$$c=\sup_{x\geq 0}\frac{f(x)}{g(x)}.$$

To generate random values $\lambda$ according to $\mathbb{P}\left(\lambda|\mathbf{N}^{\textrm{MNO}};\mathbf{N}^{\textrm{REG}}\right)$ we generate values according to $g(\lambda)$, and values $v$ according to $\textrm{Unif}(0,1)$ so that we accept those $\lambda$ such that $v\leq \frac{f(\lambda)}{c\cdot g(\lambda)}$.

To generate random values $N$ according to $\mathbb{P}\left(N|N^{\textrm{MNO}};N^{\textrm{REG}}\right)$ we generate values $\lambda$ and then the corresponding values $N$ according to the Poisson distribution with parameter $\lambda$.


# Illustrative examples: toy simulations

Let us illustrate these considerations with concrete examples. We will proceed from the simplest case to the actual data collected during the SGA-1  passing through some simulated data sets.


## The prior distributions

For the model we will make use of the following priors.

### Uniform
This is a well known distribution. For $u=\frac{\alpha}{\alpha + \beta}$ the support can be $[0,1]$ or any subinterval therein. For example, we can assume that the local market share in a territorial cell lies between a minimum value $u_{m}$ and a maximum value $u_{M}$. This is a safe way to minimally incorporate some weak information into the model.

For $v=\alpha + \beta$ we can also use prior uniform distributions with support in large intervals $[v_{m}, v_{M}]$ of possible minimum and maximum values for the cell size.


### Triangular
This can be considered a slight variant of the preceding priors. Instead of having a flat distribution along the support, we may single out a mode so that we have probability density functions as 

```{r, echo = TRUE}
library(ggplot2)
dtriang <- function(x, xMin, xMax, xMode){
  
  n <- length(x)
  if (length(xMin) == 1) xMin <- rep(xMin, n)
  if (length(xMax) == 1) xMax <- rep(xMax, n)
  if (length(xMode) == 1) xMode <- rep(xMode, n)
  
  output <- x
  output[x <= xMin] <- 0
  output[x >= xMax] <- 0
  range1 <- (x > xMin & x <= xMode)
  output[range1] <- (2 * (x[range1] - xMin[range1])) / ((xMax[range1] - xMin[range1]) * (xMode[range1] - xMin[range1]))
  range2 <- (x >= xMode & x < xMax)
  output[range2] <- (2 * (xMax[range2] - x[range2])) / ((xMax[range2] - xMin[range2]) * (xMax[range2] - xMode[range2]))
  return(output)
}

x <- seq(0.10, 0.65, by = 0.01)
y <- dtriang(x, xMin = 0.10, xMax = 0.65, xMode = 0.32)
df <- data.frame(x = x, y= y)
ggplot(df, aes(x, y)) + geom_line() + scale_x_continuous(limits = c(0, 1)) + xlab('u') + ylab('Probability Density')
```

Triangular distributions can be easily generated. They can be used for modelling the local market shares $u$, the cell size $v$ and the hyperparameter $\lambda$.

### Gamma distribution
The gamma distribution is another choice for modelling both the cell size $v$ and the hyperparameter $\lambda$.The reasoning is common to both cases. We can assume a parameterization $\Gamma(\alpha + 1, \xi^{*} / \alpha)$, where $\xi^{*}$ stands for the mode of the modelled variable ($v$ or $\lambda$) and $\alpha> 0$ determines the degree of concentration around the mode $\xi^{*}$ (see figure below).

```{r, echo = TRUE}
alphas <- c(1, 5, 10, 100, 1000)
mode <- 35
df <- lapply(alphas, function(alpha){
  
  x <- 0:100
  y <- dgamma(x, shape = alpha + 1, scale = mode / alpha)
  z <- as.character(alpha)
  output <- data.frame(x = x, y = y, alpha = z)
  return(output)
})
df <- Reduce(rbind, df)
ggplot(df, aes(x, y, col = alpha, group = alpha)) + geom_line(aes(linetype = alpha)) + scale_x_continuous(limits = c(0, 100)) + xlab('') + ylab('')
```

The choice of $\alpha$ must be guided by the data themselves. Thus we do not propose concrete methods until we practise the simulations.

Let us remind that we are combining both aggregated mobile phone data and official data at a given time instant to infer the actual population size in each cell. Official data for a concrete time instant will certainly not represent the exact population size of each cell since both data sources work at very different time scales. However, we can assume that appropriately choosing the time instant we can expect a high correlation between the actual size and official data (see e.g. @Q2016). 


In this sense, for the simulated data that we will generate, for a given actual true value $N^{0}_{i}$ we will simulate a population register value $N^{\textrm{REG}_{i}}\simeq \lfloor N(\mu = N^{0}_{i}, \sigma = 10\%\cdot N^{0}_{i})\rfloor$. For the corresponding number of individuals detected through the mobile network, we will generate it assuming a proportion of detected individuals randomly between $15\%$ and $40\%$ as realistic figures (see e.g. @WP5Del11 to compare with market shares as an approximation to these figures).

## One cell

Since the treatment of all cells is independent of each other, it is fundamental to get acquainted with the estimation process for a single cell. We investigate different combinations of priors and numerical regimes for $N^{\textrm{MNO}}$ and $N^{\textrm{REG}}$. In all cases we assume a priori $f_{3}\simeq\Gamma\left(\alpha + 1, \frac{N^{\textrm{Reg}}}{\alpha}\right)$.

Firstly, we will investigate how the estimates vary in different realizations of the estimation procedure. Let us consider a true population size of $N^{(0)} = 100$. The population register gives $N^{\textrm{Reg}}=97$ assuming an error of $3\%$. Let us also consider the number of individuals detected by the mobile network as $N^{\textrm{MNO}} = 19$ assuming a proportion of detected individuals of around $20\%$.

For the prior distribution of the proportion of detected individuals we will assume a weakly informative distribution $f_{u}=\textrm{Unif}(u_{m}, u_{M})$ with $u_{m}=0$ and $u_{M}=0.50$. For the prior distribution of the cell size we will assume a triangular distribution with parameters $v_{m}=87$, $v_{M}=107$, and $v^{*}=97$, assuming an (unknown) error of $10\%$ over the population register size. Later on we will study the effect of diverse choices on these priors.

We compute the estimates for values of $\alpha=1,10,100,1000$ to observe the effect of the amount of uncertainty in the population size (from more uncertainty to less uncertainty, respectively).

```{r, cache = TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
# the package could be installed from GitHub
# devtools::install_github('MobilePhoneESSnetBigData/pestim')
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
fu <- list('unif', xMin = 0, xMax = 0.50)
fv <- list('triang', xMin = 87, xMax = 107, xMode = 97)
alphaSeq <- c(1, 10, 100, 1000)
flambdaList <- list()
for (alpha in alphaSeq){
  
  flambdaList[[as.character(alpha)]] <- list('gamma', shape = 1 + alpha, scale = nReg / alpha)
  
}
nSim <- 100
results <- lapply(alphaSeq, function(alpha){
  
  flambda <- flambdaList[[as.character(alpha)]]
  output <- replicate(nSim, postN0(nMNO, nReg, fu, fv, flambda))
  output <- as.data.table(t(matrix(unlist(output), nrow = 3)))
  setnames(output, c('postMean', 'postMedian', 'postMode'))
  output[, sim := 1:nSim]
  output <- melt(output, id.vars = 'sim')
  output[, 'alpha' := alpha]
  return(output)
  
})
names(results) <- alphaSeq
results <- rbindlist(results)
ggplot(results, aes(x = variable, y = value)) + 
  geom_boxplot() + facet_grid(. ~ alpha) + 
  xlab('') + ylab('') + 
  geom_hline(yintercept = nReg) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5))
```

We can observe how the more precise the prior value of $\lambda = N^{\textrm{Reg}}$ is, the more precise the final estimate around $N^{\textrm{Reg}}$ will result. Notice how this final estimate inherits the original difference between $N^{(0)}$ and $N^{\textrm{Reg}}$, as expected (we are not modelling the values coming from the population register).

In the subsequent sections we will just compute one single estimate and vary the diverse prior parameters to study the effect.

### $f_{1}\simeq\textrm{Unif}(u_{m}, u_{M})$, $f_{2}\simeq\textrm{Unif}(N_{m}, N_{M})$

Let us now consider a range of values for the hyperparameters to observe the effects on the final estimate. We will choose $\alpha = 1$ as a weakly informative choice\footnote{The value of $\alpha$ will be later on better motivated when having data from several cells.}.

For the intervals $(u_{m}, u_{M})$ we will choose as centres of the intervals the natural value $N^{\textrm{MNO}} / N^{\textrm{Reg}}$. As radii, we will progressively shorten the intervals starting from $r_{1}=min(N^{\textrm{MNO}} / N^{\textrm{Reg}}, 1- N^{\textrm{MNO}} / N^{\textrm{Reg}})$ down to $0.005$.

For the intervals $(N_{m}, N_{M})$ we will choose as centres of the intervals the natural value $N^{\textrm{Reg}}$. As radii, we will progressively shorten the intervals starting from $R_{1}=\lfloor 0.25\cdot N^{\textrm{Reg}}\rfloor$ down to $1$.

The relative bias $\frac{\hat{N}-N^{\textrm{Reg}}}{N^{\textrm{Reg}}} \cdot 100$ for the posterior mean, median and mode estimates, respectively, for each pair of interval lengths $(u_{M}-u_{m}, N_{M} - N_{m})$ are:

```{r, eval=TRUE, cache=TRUE, echo = TRUE}
library(data.table)
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
nPar <- 10
radShares <- seq(from = nMNO / nReg, to = 0.005, length.out = nPar)
radPopSizes <- round(seq(from = 0.25 * nReg, to = 1, length.out = nPar))
alpha <- 1
flambda <- list('gamma', shape = alpha + 1, scale = nReg / alpha)
results.Mean <- matrix(NA, ncol = nPar, nrow = nPar)
results.Median <- matrix(NA, ncol = nPar, nrow = nPar)
results.Mode <- matrix(NA, ncol = nPar, nrow = nPar)
for (radShare.index in seq(along = radShares)) {
  for (radPopSize.index in seq(along = radPopSizes)) {
    
    um <- nMNO / nReg - radShares[radShare.index]
    uM <- nMNO / nReg + radShares[radShare.index]
    fu <- list('unif', xMin = um, xMax = uM)
    
    Nm <- nReg - radPopSizes[radPopSize.index]
    NM <- nReg + radPopSizes[radPopSize.index]
    fv <- list('unif', xMin = Nm, xMax = NM)
    
    auxResults <- postN0(nMNO, nReg, fu, fv, flambda)
    results.Mean[radShare.index, radPopSize.index] <- auxResults[['postMean']] 
    results.Median[radShare.index, radPopSize.index] <- auxResults[['postMedian']]
    results.Mode[radShare.index, radPopSize.index] <- auxResults[['postMode']]
  }
}
rownames(results.Mean) <- round(2 * radShares, 2)
rownames(results.Median) <- round(2 * radShares, 2)
rownames(results.Mode) <- round(2 * radShares, 2)
colnames(results.Mean) <- 2 * radPopSizes
colnames(results.Median) <- 2 * radPopSizes
colnames(results.Mode) <- 2 * radPopSizes
relBias.Mean <- round((results.Mean - nReg) / nReg * 100, 1)
relBias.Median <- round((results.Median - nReg) / nReg * 100, 1)
relBias.Mode <- round((results.Mode - nReg) / nReg * 100, 1)
knitr::kable(relBias.Mean, 
             caption = 'Relative bias (%) for posterior mean estimates')
knitr::kable(relBias.Median, 
             caption = 'Relative bias (%) for posterior median estimates')
knitr::kable(relBias.Mode, 
             caption = 'Relative bias (%) for posterior mode estimates')

```


### $f_{1}\simeq\textrm{Unif}(u_{m}, u_{M})$, $f_{2}\simeq\textrm{triang}(N_{m}, N_{M}, N^{\textrm{Reg}})$

We now carry out the same computation with the prior distribution for the actual population size $f_{2}$ being a triangular distribution. Its parameters will be $N_{m}$ and $N_{M}$ as in the preceding section and the mode as $N^{*}=N^{\textrm{Reg}}$.


```{r, eval=TRUE, cache=TRUE, echo = TRUE}
library(data.table)
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
nPar <- 10
radShares <- seq(from = nMNO / nReg, to = 0.005, length.out = nPar)
radPopSizes <- round(seq(from = 0.25 * nReg, to = 1, length.out = nPar))
alpha <- 1
flambda <- list('gamma', shape = alpha + 1, scale = nReg / alpha)
results.Mean <- matrix(NA, ncol = nPar, nrow = nPar)
results.Median <- matrix(NA, ncol = nPar, nrow = nPar)
results.Mode <- matrix(NA, ncol = nPar, nrow = nPar)
for (radShare.index in seq(along = radShares)) {
  for (radPopSize.index in seq(along = radPopSizes)) {
    
    um <- nMNO / nReg - radShares[radShare.index]
    uM <- nMNO / nReg + radShares[radShare.index]
    fu <- list('unif', xMin = um, xMax = uM)
    
    Nm <- nReg - radPopSizes[radPopSize.index]
    NM <- nReg + radPopSizes[radPopSize.index]
    fv <- list('triang', xMin = Nm, xMax = NM, xMode = nReg)
    
    auxResults <- postN0(nMNO, nReg, fu, fv, flambda)
    results.Mean[radShare.index, radPopSize.index] <- auxResults[['postMean']] 
    results.Median[radShare.index, radPopSize.index] <- auxResults[['postMedian']]
    results.Mode[radShare.index, radPopSize.index] <- auxResults[['postMode']]
  }
}
rownames(results.Mean) <- round(2 * radShares, 2)
rownames(results.Median) <- round(2 * radShares, 2)
rownames(results.Mode) <- round(2 * radShares, 2)
colnames(results.Mean) <- 2 * radPopSizes
colnames(results.Median) <- 2 * radPopSizes
colnames(results.Mode) <- 2 * radPopSizes
relBias.Mean <- round((results.Mean - nReg) / nReg * 100, 1)
relBias.Median <- round((results.Median - nReg) / nReg * 100, 1)
relBias.Mode <- round((results.Mode - nReg) / nReg * 100, 1)
knitr::kable(relBias.Mean, 
             caption = 'Relative bias (%) for posterior mean estimates')
knitr::kable(relBias.Median, 
             caption = 'Relative bias (%) for posterior median estimates')
knitr::kable(relBias.Mode, 
             caption = 'Relative bias (%) for posterior mode estimates')

```


### $f_{1}\simeq\textrm{Unif}(u_{m}, u_{M})$, $f_{2}\simeq\Gamma(a + 1, \frac{N^{\textrm{Reg}}}{a})$

Again we repeat the computation now with $f_{2}\simeq\Gamma(a+1, \frac{N^{\textrm{Reg}}}{a})$ and $\log_{10}(a)=-3, -2, \dots, 2, 3$.

```{r, eval=TRUE, cache=TRUE, echo = TRUE}
library(data.table)
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
nPar <- 10
radShares <- seq(from = nMNO / nReg, to = 0.005, length.out = nPar)
aPopSizes <- 10^{seq(-3, 3, by = 1)}
alpha <- 1
flambda <- list('gamma', shape = alpha + 1, scale = nReg / alpha)
results.Mean <- matrix(NA, ncol = length(aPopSizes), nrow = length(radShares))
results.Median <- matrix(NA, ncol = length(aPopSizes), nrow = length(radShares))
results.Mode <- matrix(NA, ncol = length(aPopSizes), nrow = length(radShares))
for (radShare.index in seq(along = radShares)) {
  
  um <- nMNO / nReg - radShares[radShare.index]
  uM <- nMNO / nReg + radShares[radShare.index]
  fu <- list('unif', xMin = um, xMax = uM)
  
  for (aPopSize.index in seq(along = aPopSizes)) {
    
    fv <- list('gamma', 
               shape = aPopSizes[aPopSize.index], 
               scale = nReg / aPopSizes[aPopSize.index])
    
    auxResults <- postN0(nMNO, nReg, fu, fv, flambda)
    results.Mean[radShare.index, aPopSize.index] <- auxResults[['postMean']] 
    results.Median[radShare.index, aPopSize.index] <- auxResults[['postMedian']]
    results.Mode[radShare.index, aPopSize.index] <- auxResults[['postMode']]
  }
}
rownames(results.Mean) <- round(2 * radShares, 2)
rownames(results.Median) <- round(2 * radShares, 2)
rownames(results.Mode) <- round(2 * radShares, 2)
colnames(results.Mean) <- aPopSizes
colnames(results.Median) <- aPopSizes
colnames(results.Mode) <- aPopSizes
relBias.Mean <- round((results.Mean - nReg) / nReg * 100, 1)
relBias.Median <- round((results.Median - nReg) / nReg * 100, 1)
relBias.Mode <- round((results.Mode - nReg) / nReg * 100, 1)
knitr::kable(relBias.Mean, 
             caption = 'Relative bias (%) for posterior mean estimates')
knitr::kable(relBias.Median, 
             caption = 'Relative bias (%) for posterior median estimates')
knitr::kable(relBias.Mode, 
             caption = 'Relative bias (%) for posterior mode estimates')

```


### $f_{1}\simeq\textrm{Triang}(u_{m}, u_{M}, u^{*})$, $f_{2}\simeq\textrm{Unif}(N_{m}, N_{M})$

Once more, we repeat the computation now with $f_{1}\simeq\textrm{Triang}$ and $f_{2}\simeq\textrm{Unif}$. The hyperparameters are chosen as before.

```{r, eval=TRUE, cache=TRUE, echo = TRUE}
library(data.table)
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
nPar <- 10
radShares <- seq(from = nMNO / nReg, to = 0.005, length.out = nPar)
radPopSizes <- round(seq(from = 0.25 * nReg, to = 1, length.out = nPar))
alpha <- 1
flambda <- list('gamma', shape = alpha + 1, scale = nReg / alpha)
results.Mean <- matrix(NA, ncol = nPar, nrow = nPar)
results.Median <- matrix(NA, ncol = nPar, nrow = nPar)
results.Mode <- matrix(NA, ncol = nPar, nrow = nPar)
for (radShare.index in seq(along = radShares)) {
  
  um <- nMNO / nReg - radShares[radShare.index]
  uM <- nMNO / nReg + radShares[radShare.index]
  uMode <- nMNO / nReg
  fu <- list('triang', xMin = um, xMax = uM, xMode = uMode)
  
  for (radPopSize.index in seq(along = radPopSizes)) {
    
    Nm <- nReg - radPopSizes[radPopSize.index]
    NM <- nReg + radPopSizes[radPopSize.index]
    fv <- list('unif', xMin = Nm, xMax = NM)
    
    auxResults <- postN0(nMNO, nReg, fu, fv, flambda)
    results.Mean[radShare.index, radPopSize.index] <- auxResults[['postMean']] 
    results.Median[radShare.index, radPopSize.index] <- auxResults[['postMedian']]
    results.Mode[radShare.index, radPopSize.index] <- auxResults[['postMode']]
  }
}
rownames(results.Mean) <- round(2 * radShares, 2)
rownames(results.Median) <- round(2 * radShares, 2)
rownames(results.Mode) <- round(2 * radShares, 2)
colnames(results.Mean) <- 2 * radPopSizes
colnames(results.Median) <- 2 * radPopSizes
colnames(results.Mode) <- 2 * radPopSizes
relBias.Mean <- round((results.Mean - nReg) / nReg * 100, 1)
relBias.Median <- round((results.Median - nReg) / nReg * 100, 1)
relBias.Mode <- round((results.Mode - nReg) / nReg * 100, 1)
knitr::kable(relBias.Mean, 
             caption = 'Relative bias (%) for posterior mean estimates')
knitr::kable(relBias.Median, 
             caption = 'Relative bias (%) for posterior median estimates')
knitr::kable(relBias.Mode, 
             caption = 'Relative bias (%) for posterior mode estimates')

```


### $f_{1}\simeq\textrm{Triang}(u_{m}, u_{M}, u^{*})$, $f_{2}\simeq\textrm{Triang}(N_{m}, N_{M}, N^{\textrm{Reg}})$

Now both prior distributions are triangular with the same choice of hyperparameters are before.

```{r, eval=TRUE, cache=TRUE, echo = TRUE}
library(data.table)
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
nPar <- 10
radShares <- seq(from = nMNO / nReg, to = 0.005, length.out = nPar)
radPopSizes <- round(seq(from = 0.25 * nReg, to = 1, length.out = nPar))
alpha <- 1
flambda <- list('gamma', shape = alpha + 1, scale = nReg / alpha)
results.Mean <- matrix(NA, ncol = nPar, nrow = nPar)
results.Median <- matrix(NA, ncol = nPar, nrow = nPar)
results.Mode <- matrix(NA, ncol = nPar, nrow = nPar)
for (radShare.index in seq(along = radShares)) {
  
  um <- nMNO / nReg - radShares[radShare.index]
  uM <- nMNO / nReg + radShares[radShare.index]
  uMode <- nMNO / nReg
  fu <- list('triang', xMin = um, xMax = uM, xMode = uMode)
  
  for (radPopSize.index in seq(along = radPopSizes)) {
    
    Nm <- nReg - radPopSizes[radPopSize.index]
    NM <- nReg + radPopSizes[radPopSize.index]
    Nmode <- nReg
    fv <- list('triang', xMin = Nm, xMax = NM, xMode = nReg)
    
    auxResults <- postN0(nMNO, nReg, fu, fv, flambda)
    results.Mean[radShare.index, radPopSize.index] <- auxResults[['postMean']] 
    results.Median[radShare.index, radPopSize.index] <- auxResults[['postMedian']]
    results.Mode[radShare.index, radPopSize.index] <- auxResults[['postMode']]
  }
}
rownames(results.Mean) <- round(2 * radShares, 2)
rownames(results.Median) <- round(2 * radShares, 2)
rownames(results.Mode) <- round(2 * radShares, 2)
colnames(results.Mean) <- 2 * radPopSizes
colnames(results.Median) <- 2 * radPopSizes
colnames(results.Mode) <- 2 * radPopSizes
relBias.Mean <- round((results.Mean - nReg) / nReg * 100, 1)
relBias.Median <- round((results.Median - nReg) / nReg * 100, 1)
relBias.Mode <- round((results.Mode - nReg) / nReg * 100, 1)
knitr::kable(relBias.Mean, 
             caption = 'Relative bias (%) for posterior mean estimates')
knitr::kable(relBias.Median, 
             caption = 'Relative bias (%) for posterior median estimates')
knitr::kable(relBias.Mode, 
             caption = 'Relative bias (%) for posterior mode estimates')

```


### $f_{1}\simeq\textrm{Triang}(u_{m}, u_{M}, u^{*})$, $f_{2}\simeq\Gamma(a + 1, \frac{N^{\textrm{Reg}}}{a})$

Finally we combine a triangular distribution for $f_{1}$ and a gamma distribution for $f_{2}$.

```{r, eval=TRUE, cache=TRUE, echo = TRUE}
library(data.table)
library(pestim)
nReg <- 5 #97
nMNO <- 2 #19
nPar <- 10
radShares <- seq(from = nMNO / nReg, to = 0.005, length.out = nPar)
aPopSizes <- 10^{seq(-3, 3, by = 1)}
alpha <- 1
flambda <- list('gamma', shape = alpha + 1, scale = nReg / alpha)
results.Mean <- matrix(NA, ncol = length(aPopSizes), nrow = length(radShares))
results.Median <- matrix(NA, ncol = length(aPopSizes), nrow = length(radShares))
results.Mode <- matrix(NA, ncol = length(aPopSizes), nrow = length(radShares))
for (radShare.index in seq(along = radShares)) {
  
  um <- nMNO / nReg - radShares[radShare.index]
  uM <- nMNO / nReg + radShares[radShare.index]
  uMode <- nMNO / nReg
  fu <- list('triang', xMin = um, xMax = uM, xMode = uMode)
  
  for (aPopSize.index in seq(along = aPopSizes)) {
    
    fv <- list('gamma', 
               shape = aPopSizes[aPopSize.index], 
               scale = nReg / aPopSizes[aPopSize.index])
    
    auxResults <- postN0(nMNO, nReg, fu, fv, flambda)
    results.Mean[radShare.index, aPopSize.index] <- auxResults[['postMean']] 
    results.Median[radShare.index, aPopSize.index] <- auxResults[['postMedian']]
    results.Mode[radShare.index, aPopSize.index ] <- auxResults[['postMode']]
  }
}
rownames(results.Mean) <- round(2 * radShares, 2)
rownames(results.Median) <- round(2 * radShares, 2)
rownames(results.Mode) <- round(2 * radShares, 2)
colnames(results.Mean) <- aPopSizes
colnames(results.Median) <- aPopSizes
colnames(results.Mode) <- aPopSizes
relBias.Mean <- round((results.Mean - nReg) / nReg * 100, 1)
relBias.Median <- round((results.Median - nReg) / nReg * 100, 1)
relBias.Mode <- round((results.Mode - nReg) / nReg * 100, 1)
knitr::kable(relBias.Mean, 
             caption = 'Relative bias (%) for posterior mean estimates')
knitr::kable(relBias.Median, 
             caption = 'Relative bias (%) for posterior median estimates')
knitr::kable(relBias.Mode, 
             caption = 'Relative bias (%) for posterior mode estimates')

```

## Several cells
Since by design the estimation in each cell is independent of each other, the process for a grid of cells only entails longer computation times. However, having more information allows us to seek for more objective criteria to set the prior information.

Regarding the prior distribution for the proportions of detected individuals $u_{i}$ we can consider the ratios $\frac{N_{i}^{\textrm{MNO}}}{N^{\textrm{Reg}}_{i}}$ as an initial guess as a highly probable value whose uncertainty will depend both on the process to obtain $N_{i}^{\textrm{MNO}}$ (in the preprocessing and aggregation stages of the whole process for mobile phone data; not considered here) and on the process to compile the population register figures $N_{i}^{\textrm{Reg}}$ (with non-sampling error considerations like measurement errors, processing errors, coverage, \dots). Any of the three prior distributions (uniform, triangular, gamma) can be used to express this uncertainty around $\frac{N_{i}^{\textrm{MNO}}}{N^{\textrm{Reg}}_{i}}$ and, if necessary, even more complex alternative possibilities can be built.

Regarding the prior distribution for the local cell size $v_{i}$ we can make exactly similar considerations around the value $N_{i}^{\textrm{Reg}}$ for each cell $i$ now only focusing on the process of construction of the population register.

Finally, for the prior distribution for the parameter $\lambda_{i}$ we must clearly state how critical this choice is. If we choose $\alpha_{i}\gg 1$, we are expressing a high confidence on our population register as the true population at this instant of time. In this sense it seems advisable to be conservative and choose low values so that we do not artificially  \textquotedblleft force\textquotedblright\ the final estimates to be close to $N_{i}^{\textrm{Reg}}$. In the choice of $\alpha_{i}$ in the multiple cell case, however, we can make use of the grid construction and the distribution of $N_{i}^{\textrm{Reg}}$ to propose some prior values. The variance of $\Gamma(\alpha_{i} + 1, \frac{N^{\textrm{Reg}}_{i}}{\alpha_{i}})$ is $\frac{\alpha_{i} + 1}{\alpha_{i}^{2}}\cdot N^{\textrm{Reg}}_{i}$. Thus, under the assumption of having a regular grid over the population, we can equate $\frac{\alpha_{i} + 1}{\alpha_{i}^{2}}\cdot N^{\textrm{Reg}}=\frac{1}{N_{cells} - 1}\sum_{i=1}^{N_{cell}}\left(N_{i}^{\textrm{Reg}} - \bar{N}^{\textrm{Reg}}\right)^{2}$
 to have a first value (upper bound) for $\alpha\leq\min_{i}\alpha_{i}$. In case of an irregular grid (in terms of population size), we can think of some clustering analysis to carry out the same procedure. 
 
 In any case, we claim that one can envisage objective estimation processes for the prior hyperparameters so that final estimates are not based upon dangerously subjective
beliefs.  In our view, this is another indication of why having knowledge about the preprocessing and aggregation phases in the whole process is important to arrive at final objective values to be considered official figures.


### $f_{1}\simeq\textrm{Unif}(u_{m,i}, u_{M,i})$, $f_{2}\simeq\textrm{Unif}(N_{m,i}, N_{M,i})$

Let us now consider a range of values for the hyperparameters to observe the effects on the final estimate. For the intervals $(u_{m,i}, u_{M,i})$ we will choose as centres of the intervals the natural values $N^{\textrm{MNO}}_{i} / N^{\textrm{Reg}}_{i}$. As radii, we will progressively shorten the intervals starting from $r_{1,i}=min(N^{\textrm{MNO}}_{i} / N^{\textrm{Reg}}_{i}, 1- N^{\textrm{MNO}}_{i} / N^{\textrm{Reg}}_{i})$ down to $0.005$.

For the intervals $(N_{m,i}, N_{M,i})$ we will choose as centres of the intervals the natural values $N^{\textrm{Reg}}_{i}$. As radii, we will progressively shorten the intervals starting from $R_{1,i}=\lfloor 0.25\cdot N^{\textrm{Reg}}_{i}\rfloor$ down to $1$.

The distribution of the relative bias $\frac{\hat{N}_{i}-N^{\textrm{Reg}}_{i}}{N^{\textrm{Reg}}} \cdot 100$ for the posterior mean, median and mode estimates, respectively, for all pairs of interval lengths $(u_{M,i}-u_{m,i}, N_{M,i} - N_{m,i})$ and all cells ($N_{c} = 50$) are:

```{r, eval=FALSE, cache=TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
library(pestim)
nCell <- 50
#nReg <- round(rnorm(nCell, 71, 3))
nReg <- round(rnorm(nCell, 25, 3))
#nMNO <- round(rnorm(nCell, 19, 2))
nMNO <- round(rnorm(nCell, 15, 2))
nPar <- 5
radShares <- lapply(1:nCell, function(i){
  seq(from = (nMNO / nReg)[i], to = 0.005, length.out = nPar)
})
radPopSizes <- lapply(1:nCell, function(i){
  round(seq(from = 0.25 * nReg[i], to = 1, length.out = nPar))
})
varnReg <- var(nReg)
alphaBound <- sapply(1:nCell, function(i){
  0.5 * (nReg[i] / varnReg + sqrt((nReg[i] / varnReg)^2 + 4 * nReg[i] / varnReg))
})
alpha <- min(alphaBound)
results.Mean <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Median <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Mode <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
relBias.Mean <- list()
relBias.Median <- list()
relBias.Mode <- list()
for (i in 1:nCell){
  #print(paste0('i=', i))
  for (radShare.index in seq(along = radShares[[i]])) {
    #print(paste0('radShare.index=', radShare.index))
    for (radPopSize.index in seq(along = radPopSizes[[i]])) {
      #print(paste0('radPopSize.index=', radPopSize.index))
      um <- nMNO[[i]] / nReg[[i]] - radShares[[i]][radShare.index]
      uM <- nMNO[[i]] / nReg[[i]] + radShares[[i]][radShare.index]
      fu <- list('unif', xMin = um, xMax = uM)
      
      Nm <- nReg[[i]] - radPopSizes[[i]][radPopSize.index]
      NM <- nReg[[i]] + radPopSizes[[i]][radPopSize.index]
      fv <- list('unif', xMin = Nm, xMax = NM)
      flambda <- list('gamma', shape = alpha + 1, scale = nReg[[i]] / alpha)
      
      auxResults <- postN0(nMNO[[i]], nReg[[i]], fu, fv, flambda)
      results.Mean[[i]][radShare.index, radPopSize.index] <- auxResults[['postMean']] 
      results.Median[[i]][radShare.index, radPopSize.index] <- auxResults[['postMedian']]
      results.Mode[[i]][radShare.index, radPopSize.index] <- auxResults[['postMode']]
    }
  }

  rownames(results.Mean[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Median[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Mode[[i]]) <- round(2 * radShares[[i]], 2)
  colnames(results.Mean[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Median[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Mode[[i]]) <- 2 * radPopSizes[[i]]
  relBias.Mean[[i]] <- round((results.Mean[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Median[[i]] <- round((results.Median[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Mode[[i]] <- round((results.Mode[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
}

parNames <- expand.grid(paste0('u', 1:5), paste0('v', 1:5))
colnames(parNames) <- c('u', 'v')

relBias.Mean.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Median.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Mode.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
for (i in 1:nCell){
  
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mean[[i]]))
 relBias.Mean.df <- rbind(relBias.Mean.df, aux)

 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Median[[i]]))
 relBias.Median.df <- rbind(relBias.Median.df, aux)
 
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mode[[i]]))
 relBias.Mode.df <- rbind(relBias.Mode.df, aux)
}
ggplot(relBias.Mean.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mean Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Median.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Median Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Mode.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mode Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

```

<figure>
  <img src="img/631_1.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/631_2.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/631_3.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

### $f_{1}\simeq\textrm{Unif}(u_{m}, u_{M})$, $f_{2}\simeq\textrm{triang}(N_{m}, N_{M}, N^{\textrm{Reg}})$

We now carry out the same computation with the prior distribution for the actual population size $f_{2}$ being a triangular distribution. Its parameters will be $N_{m}$ and $N_{M}$ as in the preceding section and the mode as $N^{*}=N^{\textrm{Reg}}$.

```{r, eval=FALSE, cache=TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
library(pestim)
nCell <- 10 #50
#nReg <- round(rnorm(nCell, 71, 3))
nReg <- round(rnorm(nCell, 5, 3))
#nMNO <- round(rnorm(nCell, 19, 2))
nMNO <- round(rnorm(nCell, 2, 2))
nPar <- 5
radShares <- lapply(1:nCell, function(i){
  seq(from = (nMNO / nReg)[i], to = 0.005, length.out = nPar)
})
radPopSizes <- lapply(1:nCell, function(i){
  round(seq(from = 0.25 * nReg[i], to = 1, length.out = nPar))
})
varnReg <- var(nReg)
alphaBound <- sapply(1:nCell, function(i){
  0.5 * (nReg[i] / varnReg + sqrt((nReg[i] / varnReg)^2 + 4 * nReg[i] / varnReg))
})
alpha <- min(alphaBound)
results.Mean <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Median <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Mode <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
relBias.Mean <- list()
relBias.Median <- list()
relBias.Mode <- list()
for (i in 1:nCell){
  #print(paste0('i=', i))
  for (radShare.index in seq(along = radShares[[i]])) {
    #print(paste0('radShare.index=', radShare.index))
    for (radPopSize.index in seq(along = radPopSizes[[i]])) {
      #print(paste0('radPopSize.index=', radPopSize.index))
      um <- nMNO[[i]] / nReg[[i]] - radShares[[i]][radShare.index]
      uM <- nMNO[[i]] / nReg[[i]] + radShares[[i]][radShare.index]
      fu <- list('unif', xMin = um, xMax = uM)
      
      Nm <- nReg[[i]] - radPopSizes[[i]][radPopSize.index]
      NM <- nReg[[i]] + radPopSizes[[i]][radPopSize.index]
      fv <- list('triang', xMin = Nm, xMax = NM, xMode = nReg[[i]])
      flambda <- list('gamma', shape = alpha + 1, scale = nReg[[i]] / alpha)
      
      auxResults <- postN0(nMNO[[i]], nReg[[i]], fu, fv, flambda)
      results.Mean[[i]][radShare.index, radPopSize.index] <- auxResults[['postMean']] 
      results.Median[[i]][radShare.index, radPopSize.index] <- auxResults[['postMedian']]
      results.Mode[[i]][radShare.index, radPopSize.index] <- auxResults[['postMode']]
    }
  }

  rownames(results.Mean[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Median[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Mode[[i]]) <- round(2 * radShares[[i]], 2)
  colnames(results.Mean[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Median[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Mode[[i]]) <- 2 * radPopSizes[[i]]
  relBias.Mean[[i]] <- round((results.Mean[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Median[[i]] <- round((results.Median[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Mode[[i]] <- round((results.Mode[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
}

parNames <- expand.grid(paste0('u', 1:5), paste0('v', 1:5))
colnames(parNames) <- c('u', 'v')

relBias.Mean.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Median.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Mode.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
for (i in 1:nCell){
  
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mean[[i]]))
 relBias.Mean.df <- rbind(relBias.Mean.df, aux)

 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Median[[i]]))
 relBias.Median.df <- rbind(relBias.Median.df, aux)
 
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mode[[i]]))
 relBias.Mode.df <- rbind(relBias.Mode.df, aux)
}
ggplot(relBias.Mean.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mean Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Median.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Median Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Mode.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mode Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

```

<figure>
  <img src="img/632_1.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/632_2.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/632_3.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

### $f_{1}\simeq\textrm{Unif}(u_{m}, u_{M})$, $f_{2}\simeq\Gamma(a + 1, \frac{N^{\textrm{Reg}}}{a})$

Again we repeat the computation now with $f_{2}\simeq\Gamma(a+1, \frac{N^{\textrm{Reg}}}{a})$ and $\log_{10}(a)=-3, -2, \dots, 2, 3$.

```{r, eval=FALSE, cache=TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
library(pestim)
nCell <- 50
nReg <- round(rnorm(nCell, 71, 3))
varnReg <- var(nReg)
nMNO <- round(rnorm(nCell, 19, 2))
nPar <- 5
radShares <- lapply(1:nCell, function(i){
  seq(from = (nMNO / nReg)[i], to = 0.005, length.out = nPar)
})
alphaBound <- sapply(1:nCell, function(i){
  0.5 * (nReg[i] / varnReg + sqrt((nReg[i] / varnReg)^2 + 4 * nReg[i] / varnReg))
})
alphaMin <- min(alphaBound)
aPopSizes <- seq(1000, to = alphaMin, length.out = nPar)

results.Mean <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Median <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Mode <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
relBias.Mean <- list()
relBias.Median <- list()
relBias.Mode <- list()
for (i in 1:nCell){
  #print(paste0('i=', i))
  for (radShare.index in seq(along = radShares[[i]])) {
    #print(paste0('radShare.index=', radShare.index))
    for (aPopSize.index in seq(along = aPopSizes)) {
      #print(paste0('aPopSize.index=', aPopSize.index))
      um <- nMNO[[i]] / nReg[[i]] - radShares[[i]][radShare.index]
      uM <- nMNO[[i]] / nReg[[i]] + radShares[[i]][radShare.index]
      fu <- list('unif', xMin = um, xMax = uM)
      
      alpha <- aPopSizes[aPopSize.index]
      fv <- list('gamma', 
               shape = aPopSizes[aPopSize.index] + 1, 
               scale = nReg[[i]] / aPopSizes[aPopSize.index])
      flambda <- list('gamma', shape = alphaMin + 1, scale = nReg[[i]] / alphaMin)
      
      auxResults <- postN0(nMNO[[i]], nReg[[i]], fu, fv, flambda)
      results.Mean[[i]][radShare.index, aPopSize.index] <- auxResults[['postMean']] 
      results.Median[[i]][radShare.index, aPopSize.index] <- auxResults[['postMedian']]
      results.Mode[[i]][radShare.index, aPopSize.index] <- auxResults[['postMode']]
    }
  }

  rownames(results.Mean[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Median[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Mode[[i]]) <- round(2 * radShares[[i]], 2)
  colnames(results.Mean[[i]]) <- aPopSizes
  colnames(results.Median[[i]]) <- aPopSizes
  colnames(results.Mode[[i]]) <- aPopSizes
  relBias.Mean[[i]] <- round((results.Mean[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Median[[i]] <- round((results.Median[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Mode[[i]] <- round((results.Mode[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
}

parNames <- expand.grid(paste0('u', 1:nPar), paste0('v', 1:nPar))
colnames(parNames) <- c('u', 'v')

relBias.Mean.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Median.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Mode.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
for (i in 1:nCell){
  
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mean[[i]]))
 relBias.Mean.df <- rbind(relBias.Mean.df, aux)

 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Median[[i]]))
 relBias.Median.df <- rbind(relBias.Median.df, aux)
 
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mode[[i]]))
 relBias.Mode.df <- rbind(relBias.Mode.df, aux)
}
ggplot(relBias.Mean.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mean Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Median.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Median Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Mode.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mode Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

```

<figure>
  <img src="img/633_1.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/633_2.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/633_3.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>



### $f_{1}\simeq\textrm{Triang}(u_{m}, u_{M}, u^{*})$, $f_{2}\simeq\textrm{Unif}(N_{m}, N_{M})$

Once more, we repeat the computation now with $f_{1}\simeq\textrm{Triang}$ and $f_{2}\simeq\textrm{Unif}$. The hyperparameters are chosen as before.

```{r, eval=FALSE, cache=TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
library(pestim)
nCell <- 50
nReg <- round(rnorm(nCell, 71, 3))
nMNO <- round(rnorm(nCell, 19, 2))
nPar <- 5
radShares <- lapply(1:nCell, function(i){
  seq(from = (nMNO / nReg)[i], to = 0.005, length.out = nPar)
})
radPopSizes <- lapply(1:nCell, function(i){
  round(seq(from = 0.25 * nReg[i], to = 1, length.out = nPar))
})
varnReg <- var(nReg)
alphaBound <- sapply(1:nCell, function(i){
  0.5 * (nReg[i] / varnReg + sqrt((nReg[i] / varnReg)^2 + 4 * nReg[i] / varnReg))
})
alpha <- min(alphaBound)
results.Mean <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Median <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Mode <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
relBias.Mean <- list()
relBias.Median <- list()
relBias.Mode <- list()
for (i in 1:nCell){
  #print(paste0('i=', i))
  for (radShare.index in seq(along = radShares[[i]])) {
    #print(paste0('radShare.index=', radShare.index))
    for (radPopSize.index in seq(along = radPopSizes[[i]])) {
      #print(paste0('radPopSize.index=', radPopSize.index))
      um <- nMNO[[i]] / nReg[[i]] - radShares[[i]][radShare.index]
      uM <- nMNO[[i]] / nReg[[i]] + radShares[[i]][radShare.index]
      uMode <- nMNO[[i]] / nReg[[i]]
      fu <- list('triang', xMin = um, xMax = uM, xMode = uMode)
  
      Nm <- nReg[[i]] - radPopSizes[[i]][radPopSize.index]
      NM <- nReg[[i]] + radPopSizes[[i]][radPopSize.index]
      fv <- list('unif', xMin = Nm, xMax = NM)
      
      flambda <- list('gamma', shape = alpha + 1, scale = nReg[[i]] / alpha)
      
      auxResults <- postN0(nMNO[[i]], nReg[[i]], fu, fv, flambda)
      results.Mean[[i]][radShare.index, radPopSize.index] <- auxResults[['postMean']] 
      results.Median[[i]][radShare.index, radPopSize.index] <- auxResults[['postMedian']]
      results.Mode[[i]][radShare.index, radPopSize.index] <- auxResults[['postMode']]
    }
  }

  rownames(results.Mean[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Median[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Mode[[i]]) <- round(2 * radShares[[i]], 2)
  colnames(results.Mean[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Median[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Mode[[i]]) <- 2 * radPopSizes[[i]]
  relBias.Mean[[i]] <- round((results.Mean[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Median[[i]] <- round((results.Median[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Mode[[i]] <- round((results.Mode[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
}

parNames <- expand.grid(paste0('u', 1:5), paste0('v', 1:5))
colnames(parNames) <- c('u', 'v')

relBias.Mean.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Median.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Mode.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
for (i in 1:nCell){
  
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mean[[i]]))
 relBias.Mean.df <- rbind(relBias.Mean.df, aux)

 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Median[[i]]))
 relBias.Median.df <- rbind(relBias.Median.df, aux)
 
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mode[[i]]))
 relBias.Mode.df <- rbind(relBias.Mode.df, aux)
}
ggplot(relBias.Mean.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mean Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Median.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Median Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Mode.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mode Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

```

<figure>
  <img src="img/634_1.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/634_2.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/634_3.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>



### $f_{1}\simeq\textrm{Triang}(u_{m}, u_{M}, u^{*})$, $f_{2}\simeq\textrm{Triang}(N_{m}, N_{M}, N^{\textrm{Reg}})$

Now both prior distributions are triangular with the same choice of hyperparameters are before.

```{r, eval=FALSE, cache=TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
library(pestim)
nCell <- 50
nReg <- round(rnorm(nCell, 71, 3))
nMNO <- round(rnorm(nCell, 19, 2))
nPar <- 5
radShares <- lapply(1:nCell, function(i){
  seq(from = (nMNO / nReg)[i], to = 0.005, length.out = nPar)
})
radPopSizes <- lapply(1:nCell, function(i){
  round(seq(from = 0.25 * nReg[i], to = 1, length.out = nPar))
})
varnReg <- var(nReg)
alphaBound <- sapply(1:nCell, function(i){
  0.5 * (nReg[i] / varnReg + sqrt((nReg[i] / varnReg)^2 + 4 * nReg[i] / varnReg))
})
alpha <- min(alphaBound)
results.Mean <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Median <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Mode <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
relBias.Mean <- list()
relBias.Median <- list()
relBias.Mode <- list()
for (i in 1:nCell){
  #print(paste0('i=', i))
  for (radShare.index in seq(along = radShares[[i]])) {
    #print(paste0('radShare.index=', radShare.index))
    for (radPopSize.index in seq(along = radPopSizes[[i]])) {
      #print(paste0('radPopSize.index=', radPopSize.index))
      um <- nMNO[[i]] / nReg[[i]] - radShares[[i]][radShare.index]
      uM <- nMNO[[i]] / nReg[[i]] + radShares[[i]][radShare.index]
      uMode <- nMNO[[i]] / nReg[[i]]
      fu <- list('triang', xMin = um, xMax = uM, xMode = uMode)
  
      Nm <- nReg[[i]] - radPopSizes[[i]][radPopSize.index]
      NM <- nReg[[i]] + radPopSizes[[i]][radPopSize.index]
      fv <- list('triang', xMin = Nm, xMax = NM, xMode = nReg[[i]])
      
      flambda <- list('gamma', shape = alpha + 1, scale = nReg[[i]] / alpha)
      
      auxResults <- postN0(nMNO[[i]], nReg[[i]], fu, fv, flambda)
      results.Mean[[i]][radShare.index, radPopSize.index] <- auxResults[['postMean']] 
      results.Median[[i]][radShare.index, radPopSize.index] <- auxResults[['postMedian']]
      results.Mode[[i]][radShare.index, radPopSize.index] <- auxResults[['postMode']]
    }
  }

  rownames(results.Mean[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Median[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Mode[[i]]) <- round(2 * radShares[[i]], 2)
  colnames(results.Mean[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Median[[i]]) <- 2 * radPopSizes[[i]]
  colnames(results.Mode[[i]]) <- 2 * radPopSizes[[i]]
  relBias.Mean[[i]] <- round((results.Mean[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Median[[i]] <- round((results.Median[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Mode[[i]] <- round((results.Mode[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
}

parNames <- expand.grid(paste0('u', 1:5), paste0('v', 1:5))
colnames(parNames) <- c('u', 'v')

relBias.Mean.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Median.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Mode.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
for (i in 1:nCell){
  
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mean[[i]]))
 relBias.Mean.df <- rbind(relBias.Mean.df, aux)

 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Median[[i]]))
 relBias.Median.df <- rbind(relBias.Median.df, aux)
 
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mode[[i]]))
 relBias.Mode.df <- rbind(relBias.Mode.df, aux)
}
ggplot(relBias.Mean.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mean Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Median.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Median Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Mode.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mode Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

```

<figure>
  <img src="img/635_1.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/635_2.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/635_3.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>


### $f_{1}\simeq\textrm{Triang}(u_{m}, u_{M}, u^{*})$, $f_{2}\simeq\Gamma(a + 1, \frac{N^{\textrm{Reg}}}{a})$

Finally we combine a triangular distribution for $f_{1}$ and a gamma distribution for $f_{2}$.

```{r, eval=FALSE, cache=TRUE, echo = TRUE}
library(data.table)
library(ggplot2)
library(pestim)
nCell <- 50
nReg <- round(rnorm(nCell, 71, 3))
varnReg <- var(nReg)
nMNO <- round(rnorm(nCell, 19, 2))
nPar <- 5
radShares <- lapply(1:nCell, function(i){
  seq(from = (nMNO / nReg)[i], to = 0.005, length.out = nPar)
})
alphaBound <- sapply(1:nCell, function(i){
  0.5 * (nReg[i] / varnReg + sqrt((nReg[i] / varnReg)^2 + 4 * nReg[i] / varnReg))
})
alphaMin <- min(alphaBound)
aPopSizes <- seq(1000, to = alphaMin, length.out = nPar)

results.Mean <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Median <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
results.Mode <- lapply(1:nCell, function(i){matrix(NA, ncol = nPar, nrow = nPar)})
relBias.Mean <- list()
relBias.Median <- list()
relBias.Mode <- list()
for (i in 1:nCell){
  #print(paste0('i=', i))
  for (radShare.index in seq(along = radShares[[i]])) {
    #print(paste0('radShare.index=', radShare.index))
    for (aPopSize.index in seq(along = aPopSizes)) {
      #print(paste0('aPopSize.index=', aPopSize.index))
      um <- nMNO[[i]] / nReg[[i]] - radShares[[i]][radShare.index]
      uM <- nMNO[[i]] / nReg[[i]] + radShares[[i]][radShare.index]
      uMode <- nMNO[[i]] / nReg[[i]]
      fu <- list('triang', xMin = um, xMax = uM, xMode = uMode)
      
      alpha <- aPopSizes[aPopSize.index]
      fv <- list('gamma', 
               shape = aPopSizes[aPopSize.index] + 1, 
               scale = nReg[[i]] / aPopSizes[aPopSize.index])
      
      flambda <- list('gamma', shape = alphaMin + 1, scale = nReg[[i]] / alphaMin)
      
      auxResults <- postN0(nMNO[[i]], nReg[[i]], fu, fv, flambda)
      results.Mean[[i]][radShare.index, aPopSize.index] <- auxResults[['postMean']] 
      results.Median[[i]][radShare.index, aPopSize.index] <- auxResults[['postMedian']]
      results.Mode[[i]][radShare.index, aPopSize.index] <- auxResults[['postMode']]
    }
  }

  rownames(results.Mean[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Median[[i]]) <- round(2 * radShares[[i]], 2)
  rownames(results.Mode[[i]]) <- round(2 * radShares[[i]], 2)
  colnames(results.Mean[[i]]) <- aPopSizes
  colnames(results.Median[[i]]) <- aPopSizes
  colnames(results.Mode[[i]]) <- aPopSizes
  relBias.Mean[[i]] <- round((results.Mean[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Median[[i]] <- round((results.Median[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
  relBias.Mode[[i]] <- round((results.Mode[[i]] - nReg[[i]]) / nReg[[i]] * 100, 1)
}

parNames <- expand.grid(paste0('u', 1:5), paste0('v', 1:5))
colnames(parNames) <- c('u', 'v')

relBias.Mean.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Median.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
relBias.Mode.df <- data.frame(u = character(0), 
                              v = character(0), 
                              cell = character(0), 
                              N = numeric(0))
for (i in 1:nCell){
  
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mean[[i]]))
 relBias.Mean.df <- rbind(relBias.Mean.df, aux)

 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Median[[i]]))
 relBias.Median.df <- rbind(relBias.Median.df, aux)
 
 aux <- cbind(parNames, cell = as.character(i), N = as.vector(relBias.Mode[[i]]))
 relBias.Mode.df <- rbind(relBias.Mode.df, aux)
}
ggplot(relBias.Mean.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mean Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Median.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Median Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

ggplot(relBias.Mode.df, aes(x = ' ', y = N)) +
  geom_boxplot() +
  facet_grid(factor(u) ~ factor(v)) +
  xlab('Diverse combinations of prior parameters') + ylab('Mode Estimates\n') +
  ggtitle(paste0('Relative bias (%) distributions of the ', nCell, ' cells\n')) +
  theme(plot.title = element_text(face = 'bold', size = 14, hjust = 0.5))

```

<figure>
  <img src="img/636_1.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/636_2.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>

<figure>
  <img src="img/636_3.png" alt="Relative bias (%) distributions of the 50 cells" width="100%" >
</figure>


## Simulations with aggregated mobile phone data and the Spanish population register
For more realistic values (more than 3-digit figures) we have detected some needs of improvement in the implementation of the Kummer function computation and of the optimization algorithm. We will carry these simulations in a separate document as soon as these improvements have been completed.

For the time being we will illustrate in a complementary document the IT side of the preceding computations so that these tools can be used for any user upon his/her own data.


## Next steps

This document illustrates our proposal to infer population counts $N_{i,0}$ over a population partition into cells $i$ at a given time instant $t_{0}$ from the input values $(N_{i,0}^{\textrm{Reg}}, N_{i,0}^{\textrm{MNO}})$ of population counts in a population register or a similar auxiliary data source and detected by a cellular telecommunication network.

The next step is to introduce the time dimension by considering how these population counts evolve. This work is already ongoing and is based on the main working hypothesis that the displacement of individuals among territorial cells is independent of the MNO of each subscriber. Taking the matrix $N^{\textrm{MNO}}(t_{0}, t_{1})=[N_{ij}^{\textrm{MNO}}(t_{0}, t_{1})]_{1\leq i,j\leq I}$ of number of displaced subscribers from cell $i$ to cell $j$ in the time interval $[t_{0}, t_{1}]$ and the registered population counts $N_{i,0}^{\textrm{Reg}}$ as input data we propose the next hierarchical model:

\begin{align}
N_{i}(t_{1})&=\big\lfloor\sum_{j=1}^{I}p_{j\to i}(t_{0}, t_{1})N_{j}(t_{0})-\sum_{\substack{j=1\\j\neq i}}^{I}p_{i\to j}(t_{0}, t_{1})N_{i}(t_{0})\big\rfloor,\quad i=1,\dots,I\\
\mathbf{p}_{i\cdot}(t_{0}, t_{1})& \simeq \textrm{Dirichlet}(\mathbf{\alpha}_{i\cdot}(t_{0}, t_{1})),\quad i=1,\dots,I\nonumber\\
\mathbf{\alpha}_{i\cdot}&\simeq\frac{\prod_{j=1}^{I-1}f_{1}\left(\frac{\alpha_{ij}(t_{0}, t_{1})}{\alpha_{i}^{(0)}(t_{0}, t_{1})};\frac{N_{ij}^{\textrm{MNO}}(t_{0}, t_{1})}{N_{i}^{\textrm{MNO}}(t_{0})}\right)\cdot f_{2}\left(\right)}{(\alpha_{i}^{(0)}(t_{0}, t_{1}))^{I - 1}},\quad i=1,\dots,I\nonumber\\
N_{i}^{\textrm{MNO}}(t_{0})&\simeq\textrm{Bin}\left(N_{i}(t_{0}), p_{i}(t_{0})\right),\qquad N_{i}^{\textrm{MNO}}(t_{0})\perp N_{j}^{\textrm{MNO}}(t_{0}),\quad i\neq j=1,\dots,I\nonumber\\
N_{i}(t_{0})&\simeq\textrm{Po}\left(\lambda_{i}(t_{0})\right),\qquad N_{i}(t_{0})\perp N_{j}(t_{0}),\quad i\neq j=1,\dots,I\nonumber\\
p_{i}(t_{0})&\simeq\textrm{Beta}\left(\alpha_{i}(t_{0}),\beta_{i}(t_{0})\right),\qquad p_{i}(t_{0})\perp p_{j}(t_{0})\quad i\neq j=1,\dots,I\nonumber\\
\hspace*{-1cm}\left(\alpha_{i}(t_{0}), \beta_{i}(t_{0})\right)&\simeq \frac{f_{1}(\frac{\alpha_{i}(t_{0})}{\alpha_{i}(t_{0})+\beta_{i}(t_{0})}; \mathbf{N}^{\textrm{REG}}(t_{0}))\cdot f_{2}(\alpha_{i}(t_{0})+\beta_{i}(t_{0}); \mathbf{N}^{\textrm{REG}}(t_{0}))}{\alpha_{i}(t_{0})+\beta_{i}}(t_{0}),\qquad (\alpha_{i}(t_{0}),\beta_{i}(t_{0}))\perp(\alpha_{j}(t_{0}),\beta_{j}(t_{0})),\nonumber\\
\lambda_{i}(t_{0})&\simeq f_{3}(\lambda_{i}(t_{0}); N_{i}^{\textrm{REG}}(t_{0}))\quad (\lambda_{i}(t_{0}) > 0, \lambda_{i}(t_{0})\perp\lambda_{j})(t_{0}), \quad i=1,\dots,I,\nonumber
\end{align}

\noindent where $\alpha_{i}^{(0)}(t_{0}, t_{1})=\sum_{j=1}^{I}\alpha_{ij}(t_{0}, t_{1})$. The posterior probability distribution to compute is $$\hspace*{-1cm}\mathbb{P}\left(N_{i}(t_{1})\big| N^{\textrm{MNO}}(t_{0}, t_{1}), N^{\textrm{Reg}}(t_{0})\right) = \sum_{n=0}^{\infty}\mathbb{P}\left(N_{i}(t_{1})\big| N_{i}(t_{0}) = n, N^{\textrm{MNO}}(t_{0}, t_{1}), N^{\textrm{Reg}}(t_{0})\right)\mathbb{P}\left(N_{i}(t_{0})=n\big| N^{\textrm{MNO}}(t_{0}, t_{1}), N^{\textrm{Reg}}(t_{0})\right).$$

This will be undertaken in a separate document. Notice that $\mathbb{P}\left(N_{i}(t_{0})=n\big| N^{\textrm{MNO}}(t_{0}, t_{1}), N^{\textrm{Reg}}(t_{0})\right)$ is the result of the preceding proposal and $\mathbb{P}\left(N_{i}(t_{1})\big| N_{i}(t_{0}) = n, N^{\textrm{MNO}}(t_{0}, t_{1}), N^{\textrm{Reg}}(t_{0})\right)$ can be computed using simulations.



\newpage

# Appendix A: Computation of $J_{n+m, p}(N)$

The integral $J_{n+m, p}(N)$ can be computed using the residue theorem (see e.g. @BroChu03a). Applying this theorem to $g(z)=f_{N}(z)\cdot \frac{z^{p}}{\prod_{k=1}^{n+m-1}(z+k)}\cdot \log(z)$ in the closed path around the origin composed by a straight path $\gamma_{1}$ along and above the positive real axis (from $+\epsilon$ to $+R$), a counterclockwise circular path $\gamma_{R}$ at radius $R$, a straight path $\gamma_{2}$ along and below the positive real axis (from $+R$ to $+\epsilon$) and a clockwise circular path $\gamma_{\epsilon}$ at radius $\epsilon$. We place a branch cut at the positive real axis. Then it is easy to prove (via Jordan's lemma) that $\int_{\gamma_{R}}g(z)dz\to 0$ and $\int_{\gamma_{\epsilon}}g(z)dz$ when $R\to\infty$ and $\epsilon\to 0$, while 

\begin{align}
\int_{\gamma_{1}}g(z)dz&\to\int_{0}^{\infty}dx\ f_{N}(x)\frac{x^{p}}{\prod_{k=1}^{n+m-1}(x+k)}\log(x),\\
\int_{\gamma_{2}}g(z)dz&\to-\int_{0}^{\infty}dx\ f_{N}(x)\frac{x^{p}}{\prod_{k=1}^{n+m-1}(x+k)}\left(\log(x) + 2\pi i\right).
\end{align}

The poles of $g(z)$ are simple and located at $z = -k$, $k=1, \dots, n+m-1$ and the residues can be computed easily:

\begin{align}
\textrm{Res}(g, -k)&=f_{N}(-k)\cdot \frac{(-k)^{p}}{\prod_{\substack{i=1\\i\neq k}}^{n+m-1}(i-k)}\log\left(ke^{i\pi}\right)\nonumber\\
&=f_{N}(-k)\frac{(-1)^{p}k^{p}}{(-1)^{k-1}(k-1)!(n+m-1-k)!}\left(\log(k) + i\pi\right)\nonumber\\
&=f_{N}(-k)\cdot\frac{(-1)^{p-k} k^{p + 1}}{(n+m-1)!}\binom{n+m-1}{k}\left(\log(k) + i\pi\right)
\end{align}

Substituting on the residue theorem and focusing on the imaginary part of the expressions we have 

\begin{align}
-2\pi i\int_{0}^{\infty}dx\quad f_{N}(x)\frac{x^{p}}{\prod_{k=1}^{n+m-1}(x+k)}&=2\pi i\sum_{k=1}^{n+m-1}f(-k)\cdot\frac{(-1)^{p-k} k^{p+1}}{(n+m-1)!}\binom{n+m-1}{k}\left(\log(k) + i\pi\right),
\end{align}

thus arriving at

\begin{align}
J_{n+m,p}(N)=\int_{0}^{\infty}dx\quad f_{N}(x)\frac{x^{p}}{\prod_{k=1}^{n+m-1}(x+k)}&=\frac{1}{(n+m-1)!}\sum_{k=1}^{n+m-1}(-1)^{p-k-1}\binom{n+m-1}{k}f(-k)\cdot k^{p+1}\log(k).
\end{align}

Setting $n=N_{i}^{\textrm{MNO}}$, $m=n_{i}-N_{i}^{\textrm{MNO}}$ and $N=N_{i}^{\textrm{REG}}$, we have 

\begin{align}
J_{n_{i},p_{i}}(N_{i}^{\textrm{REG}})&=\frac{1}{(n_{i}-1)!}\sum_{k_{i}=1}^{n_{i}-1}(-1)^{p_{i}-k_{i}-1}\binom{n_{i}-1}{k_{i}}f_{N_{i}^{\textrm{REG}}}(-k_{i})\cdot k_{i}^{p_{i}+1}\log(k_{i}).
\end{align}



# Appendix B: computation of $a_{N, n- N}(m)$

Let us consider the following attempt to compute the numerical factors $$a_{N,n-N}(p)=\sum_{q=0}^{p}q!\left[\begin{matrix}n\\q\end{matrix}\right](p-q)!\left[\begin{matrix}n-N\\p-q\end{matrix}\right].$$

Let us consider the generating function $G_{k}(z)=\frac{(\log(1+z))^{k}}{k!}=\sum_{n=0}^{\infty}(-1)^{n-k}\left[\begin{matrix}n\\k\end{matrix}\right]\frac{z^{n}}{n!}$. Notice that $G_{k}^{(n)}(0) = (-1)^{n-k}\left[\begin{matrix}N\\s\end{matrix}\right]$. 

We apply Bruno di FaÃ 's formula (@Joh02a) to $k!G_{k}^{(n)}(z)=(\log(1+z))^{k}$:

$$\frac{d^{n}}{dz^{n}}f(g(z)) = \sum_{m=1}^{n}\sum_{\substack{k_{1},\dots,k_{n}=0\\k_{1}+2\cdot k_{2}+\dots+n\cdot k_{n}=n}}^{m}\frac{n!}{k_{1}!\dots k_{n}!}f^{(m)}(g(z))\left(\frac{g^{(1)}(z)}{1!}\right)^{k_{1}}\cdots\left(\frac{g^{(n)}(z)}{n!}\right)^{k_{n}}.$$

Now, in our case we have $f(z) = z^{k}$ and $g(z)= \log(1+z)$. Then we can write

\begin{align}
k!\left[\begin{matrix}n\\k\end{matrix}\right]= (-1)^{n-k}k!G_{k}^{(n)}(0)&=(-1)^{n-k}\sum_{m=1}^{k}\sum_{\substack{k_{1},\dots, k_{n}=0\\k_{1}+2\cdot k_{2}+\dots+ n\cdot k_{n}=n}}^{m}\frac{n!}{k_{1}!\cdots k_{n}!}\frac{k!}{(k-m)!}\delta_{km}\left(\frac{(-1)^{1-1}}{1}\right)^{k_{1}}\cdots \left(\frac{(-1)^{n-1}}{n}\right)^{k_{n}}\nonumber\\
&=(-1)^{n-k}\cdot n!\cdot k!\sum_{\substack{k_{1},\dots, k_{n}=0\\k_{1}+2\cdot k_{2}+\dots +n\cdot k_{n}=n}}^{k}\frac{1}{k_{1}!\dots k_{n}!}\left(\frac{1}{1}\right)^{k_{1}}\cdots \left(\frac{(-1)^{n-1}}{n}\right)^{k_{n}}.
\end{align}

We have found no way to further simplify this expression. Notice also that for large $n$ and $k$ as in our case, it can hardly be implemented in a computer routine (indeed, Stirling numbers are often computed using recurrence relations in $O(n^{2})$ steps). So far, no further simplification has been found. 




# Complete model

This part of the document contains a hierarchical model to estimate population counts using a combination of aggregated mobile phone data and official data both at a given time instant and along a sequence of time periods. This work completes the preceding proposal which sets the model for the initial time instant estimates (cf.\ @WP5IntDoc4). Thus, in the subsequent we show how to extend the estimates along a sequence of time instants, not just the initial one.

We provide a full view of the proposal to offer a complete view of this methodological approach. 

Our approach can be graphically represented by figure: 
<figure>
  <img src="img/Tool2.png" alt="Schematic diagram for the output intended using mobile phone and official population data" width="100%" >
</figure>




# References

Brown, J.W., and R.V. Churchill. 2003. âComplex Variables and Applications (7th Ed.).â

De Meersman, F., G. Seynaeve, M. Debusschere, P. Lusyne, P. Dewitte, Y. Baeyens, A. Wirthmann, C.
Demunter, F. Reis, and H.I. Reuter. 2016. âAssessing the Quality of Mobile Phone Data as a Source of
Statistics.â Q2016 Conference.

Devroye, L. 1986. âNon-Uniform Random Variable Generation.â Springer.

ESSnet on Big Data WP5. 2017. âCurrent Status of Access to Mobile Phone Data in the ESS.â

Gelman, A., J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. 2013. âBayesian Data
Analysis (3rd Ed).â CRC Press.

Graham, R.L., D.E. Knuth, and O. Patashnik. 1996. âConcrete Mathematics (2nd Ed.).â Addison-Wesley.

Johnson, W.P. 2002. âThe Curious History of FaÃ  Di Brunoâs Formula.â American Mathematical Monthly
109, 217-234.

Manly, B.F.J., and J.A. Navarro Alberto, eds. 2015. âIntroduction to Ecological Sampling.â CRC Press.

Royle, J.A., and R.M. Dorazio. 2008. âHierarchical Modeling and Inference in Ecology.â Academic Press.
